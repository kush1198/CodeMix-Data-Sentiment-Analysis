{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "name": "Untitled.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZPDDXI7bvm7a",
        "colab_type": "code",
        "outputId": "da5ccbce-24ed-4287-f7bf-e01a701df65b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_W-PihZewZO8",
        "colab_type": "code",
        "outputId": "b94e30ba-18a6-4ff2-92b2-bb5ff22fe057",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "cd /content/drive/My Drive/Colab Notebooks/HAN"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/HAN\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5CjD9A9BeqLf",
        "colab_type": "code",
        "outputId": "57622b70-97c1-4496-89f8-f2b239c3936d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "IIITH_Codemixed.txt  MyNormalizer.py  \u001b[0m\u001b[01;34m__pycache__\u001b[0m/  Untitled7.ipynb\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BvehAKxA7GkV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import numpy as np\n",
        "import h5py\n",
        "import pickle\n",
        "from copy import deepcopy\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential\n",
        "from keras.preprocessing import sequence\n",
        "from keras import backend as K\n",
        "from keras.layers.core import Dense, Dropout, Activation\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.layers.recurrent import LSTM, GRU\n",
        "from keras.layers.convolutional import Convolution1D, MaxPooling1D\n",
        "from keras.layers import Conv1D\n",
        "import tensorflow as tf\n",
        "\n",
        "from keras.utils import np_utils\n",
        "from MyNormalizer import token\n",
        "\n",
        "################# GLOBAL VARIABLES #####################\n",
        "\n",
        "inputdatasetfilename = 'IIITH_Codemixed.txt'\n",
        "\n",
        "\n",
        "#Data I/O formatting\n",
        "SEPERATOR = '\\t'\n",
        "DATA_COLUMN = 1\n",
        "LABEL_COLUMN = 3\n",
        "LABELS = ['0','1','2'] # 0 -> Negative, 1-> Neutral, 2-> Positive\n",
        "mapping_char2num = {}\n",
        "mapping_num2char = {}\n",
        "MAXLEN = 200\n",
        "\n",
        "#LSTM Model Parameters\n",
        "#Embedding\n",
        "MAX_FEATURES = 0\n",
        "embedding_size = 128\n",
        "# Convolution\n",
        "filter_length = 3\n",
        "nb_filter = 128\n",
        "pool_length = 3\n",
        "# LSTM\n",
        "lstm_output_size = 128\n",
        "# Training\n",
        "batch_size = 128\n",
        "number_of_epochs = 80\n",
        "numclasses = 3\n",
        "test_size = 0.2\n",
        "########################################################\n",
        "\n",
        "def parse(filename):\n",
        "\t\"\"\"\n",
        "\tPurpose -> Data I/O\n",
        "\tInput   -> Data file containing sentences and labels along with the global variables\n",
        "\tOutput  -> Sentences cleaned up in list of lists format along with the labels as a numpy array\n",
        "\t\"\"\"\n",
        "\t#Reads the files and splits data into individual lines\n",
        "\tf=open(filename,'r',encoding='utf-8')\n",
        "\tlines = f.read().lower()\n",
        "\tlines = lines.lower().split('\\n')[:-1]\n",
        "\n",
        "\tX_train = []\n",
        "\tY_train = []\n",
        "\t\n",
        "\t#Processes individual lines\n",
        "\tfor line in lines:\n",
        "\t\t# Seperator for the current dataset. Currently '\\t'. \n",
        "\t\tline = line.split(seperator)\n",
        "\t\t#Token is the function which implements basic preprocessing as mentioned in our paper\n",
        "\t\ttokenized_lines = token(line[datacol])\n",
        "\t\t\n",
        "\t\t#Creates character lists\n",
        "\t\tchar_list = []\n",
        "\t\tfor words in tokenized_lines:\n",
        "\t\t\tfor char in words:\n",
        "\t\t\t\tchar_list.append(char)\n",
        "\t\t\tchar_list.append(' ')\n",
        "\t\t#print(char_list) - Debugs the character list created\n",
        "\t\tX_train.append(char_list)\n",
        "\t\t\n",
        "\t\t#Appends labels\n",
        "\t\tif line[labelcol] == labels[0]:\n",
        "\t\t\tY_train.append(0)\n",
        "\t\tif line[labelcol] == labels[1]:\n",
        "\t\t\tY_train.append(1)\n",
        "\t\tif line[labelcol] == labels[2]:\n",
        "\t\t\tY_train.append(2)\n",
        "\t\n",
        "\t#Converts Y_train to a numpy array\t\n",
        "\tY_train = np.asarray(Y_train)\n",
        "\tassert(len(X_train) == Y_train.shape[0])\n",
        "\n",
        "\treturn [X_train,Y_train]\n",
        "\n",
        "def convert_char2num(mapping_n2c,mapping_c2n,trainwords,maxlen):\n",
        "\t\"\"\"\n",
        "\tPurpose -> Convert characters to integers, a unique value for every character\n",
        "\tInput   -> Training data (In list of lists format) along with global variables\n",
        "\tOutput  -> Converted training data along with global variables\n",
        "\t\"\"\"\n",
        "\tallchars = []\n",
        "\terrors = 0\n",
        "\n",
        "\t#Creates a list of all characters present in the dataset\n",
        "\tfor line in trainwords:\n",
        "\t\ttry:\n",
        "\t\t\tallchars = set(allchars+line)\n",
        "\t\t\tallchars = list(allchars)\n",
        "\t\texcept:\n",
        "\t\t\terrors += 1\n",
        "\n",
        "\t#print(errors) #Debugging\n",
        "\t#print(allchars) #Debugging \n",
        "\n",
        "\t#Creates character dictionaries for the characters\n",
        "\tcharno = 0\n",
        "\tfor char in allchars:\n",
        "\t\tmapping_char2num[char] = charno\n",
        "\t\tmapping_num2char[charno] = char\n",
        "\t\tcharno += 1\n",
        "\n",
        "\tassert(len(allchars)==charno) #Checks\n",
        "\n",
        "\t#Converts the data from characters to numbers using dictionaries \n",
        "\tX_train = []\n",
        "\tfor line in trainwords:\n",
        "\t\tchar_list=[]\n",
        "\t\tfor letter in line:\n",
        "\t\t\tchar_list.append(mapping_char2num[letter])\n",
        "\t\t#print(no) -- Debugs the number mappings\n",
        "\t\tX_train.append(char_list)\n",
        "\tprint(mapping_char2num)\n",
        "\tprint(mapping_num2char)\n",
        "\t#Pads the X_train to get a uniform vector\n",
        "\t#TODO: Automate the selection instead of manual input\n",
        "\tX_train = sequence.pad_sequences(X_train[:], maxlen=maxlen)\n",
        "\treturn [X_train,mapping_num2char,mapping_char2num,charno]\n",
        "\n",
        "def RNN(X_train,y_train,args):\n",
        "\t\"\"\"\n",
        "\tPurpose -> Define and train the proposed LSTM network\n",
        "\tInput   -> Data, Labels and model hyperparameters\n",
        "\tOutput  -> Trained LSTM network\n",
        "\t\"\"\"\n",
        "\t#Sets the model hyperparameters\n",
        "\t#Embedding hyperparameters\n",
        "\tmax_features = args[0]\n",
        "\tmaxlen = args[1]\n",
        "\tembedding_size = args[2]\n",
        "\t# Convolution hyperparameters\n",
        "\tfilter_length = args[3]\n",
        "\tnb_filter = args[4]\n",
        "\tpool_length = args[5]\n",
        "\t# LSTM hyperparameters\n",
        "\tlstm_output_size = args[6]\n",
        "\t# Training hyperparameters\n",
        "\tbatch_size = args[7]\n",
        "\tnb_epoch = args[8]\n",
        "\tnumclasses = args[9]\n",
        "\ttest_size = args[10] \n",
        "\n",
        "\t#Format conversion for y_train for compatibility with Keras\n",
        "\ty_train = np_utils.to_categorical(y_train, numclasses) \n",
        "\t#Train & Validation data splitting\n",
        "\tX_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=test_size, random_state=42)\n",
        "\t\n",
        "\t#Build the sequential model\n",
        "\t# Model Architecture is:\n",
        "\t# Input -> Embedding -> Conv1D+Maxpool1D -> LSTM -> LSTM -> FC-1 -> Softmaxloss\n",
        "\tprint('Build model...')\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Embedding(max_features, embedding_size, input_length=maxlen))\n",
        "\tmodel.add(Convolution1D(nb_filter=nb_filter,\n",
        "\t\t\t\t\t\t\tfilter_length=filter_length,\n",
        "\t\t\t\t\t\t\tborder_mode='valid',\n",
        "\t\t\t\t\t\t\tactivation='relu',\n",
        "\t\t\t\t\t\t\tsubsample_length=1))\n",
        "\tmodel.add(MaxPooling1D(pool_length=pool_length))\n",
        "\tmodel.add(LSTM(lstm_output_size, dropout_W=0.2, dropout_U=0.2, return_sequences=True))\n",
        "\tmodel.add(LSTM(lstm_output_size, dropout_W=0.2, dropout_U=0.2, return_sequences=False))\n",
        "\tmodel.add(Dense(numclasses))\n",
        "\tmodel.add(Activation('softmax'))\n",
        "\n",
        "\t# Optimizer is Adamax along with categorical crossentropy loss\n",
        "\tmodel.compile(loss='categorical_crossentropy',optimizer='adamax',metrics=['accuracy'])\n",
        "\t\n",
        "\n",
        "\tprint('Train...')\n",
        "\t#Trains model for 50 epochs with shuffling after every epoch for training data and validates on validation data\n",
        "\thistory=model.fit(X_train, y_train, batch_size=batch_size, shuffle=True, nb_epoch=nb_epoch,validation_data=(X_valid, y_valid))\n",
        "\treturn (model,history)\n",
        "\n",
        "\n",
        "\n",
        "def get_activations(model, layer, X_batch):\n",
        "\t\"\"\"\n",
        "\tPurpose -> Obtains outputs from any layer in Keras\n",
        "\tInput   -> Trained model, layer from which output needs to be extracted & files to be given as input\n",
        "\tOutput  -> Features from that layer \n",
        "\t\"\"\"\n",
        "\t#Referred from:- TODO: Enter the forum link from where I got this\n",
        "\tget_activations = K.function([model.layers[0].input, K.learning_phase()], [model.layers[layer].output,])\n",
        "\tactivations = get_activations([X_batch,0])\n",
        "\treturn activations\n",
        "\n",
        "def evaluate_model(X_test,y_test,model,batch_size,numclasses):\n",
        "\t\"\"\"\n",
        "\tPurpose -> Evaluate any model on the testing data\n",
        "\tInput   -> Testing data and labels, trained model and global variables\n",
        "\tOutput  -> Nil\n",
        "\t\"\"\"\n",
        "\t#Convert y_test to one-hot encoding\n",
        "\ty_test = np_utils.to_categorical(y_test, numclasses)\n",
        "\t#Evaluate the accuracies\n",
        "\tscore, acc = model.evaluate(X_test, y_test, batch_size=batch_size)\n",
        "\tprint('Test score:', score)\n",
        "\tprint('Test accuracy:', acc)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UksTFWGGDE6-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQNC-ygp7Gmn",
        "colab_type": "code",
        "outputId": "c4f21709-0d44-4f12-aa03-045e88bf06a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        }
      },
      "source": [
        "    print('Starting RNN Engine...\\nModel: Char-level LSTM.\\nParsing data files...')\n",
        "    out = parse(inputdatasetfilename)\n",
        "    global X_train\n",
        "    global y_train\n",
        "    global X_test\n",
        "    global y_test\n",
        "    X_train = out[0]\n",
        "    y_train = out[1]\n",
        "    print(y_train)\n",
        "    print('Parsing complete!')\n",
        "    print('Creating character dictionaries and format conversion in progess...')\n",
        "    out = convert_char2num(mapping_num2char,mapping_char2num,X_train,MAXLEN)\n",
        "    mapping_num2char = out[1]\n",
        "    mapping_char2num = out[2]\n",
        "    MAX_FEATURES = out[3]\n",
        "    X_train = np.asarray(out[0])\n",
        "    y_train = np.asarray(y_train).flatten()\n",
        "    print('Complete!')\n",
        "    print('Splitting data into train and test...')\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
        "    y_train=tf.keras.utils.to_categorical(y_train)\n",
        "    y_test=tf.keras.utils.to_categorical(y_test)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting RNN Engine...\n",
            "Model: Char-level LSTM.\n",
            "Parsing data files...\n",
            "[2 2 1 ... 2 2 2]\n",
            "Parsing complete!\n",
            "Creating character dictionaries and format conversion in progess...\n",
            "{'k': 0, 'p': 1, 'c': 2, 'd': 3, 'f': 4, 'y': 5, 'x': 6, ' ': 7, 's': 8, 'm': 9, 'r': 10, 'z': 11, 'e': 12, 'o': 13, 't': 14, 'w': 15, 'u': 16, 'b': 17, 'g': 18, 'q': 19, 'h': 20, 'i': 21, 'v': 22, 'n': 23, 'j': 24, 'l': 25, 'a': 26}\n",
            "{0: 'k', 1: 'p', 2: 'c', 3: 'd', 4: 'f', 5: 'y', 6: 'x', 7: ' ', 8: 's', 9: 'm', 10: 'r', 11: 'z', 12: 'e', 13: 'o', 14: 't', 15: 'w', 16: 'u', 17: 'b', 18: 'g', 19: 'q', 20: 'h', 21: 'i', 22: 'v', 23: 'n', 24: 'j', 25: 'l', 26: 'a'}\n",
            "Complete!\n",
            "Splitting data into train and test...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-QD8NwuCtQ8",
        "colab_type": "code",
        "outputId": "ca1528da-b089-46c6-b58d-9f8b1f0006a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "source": [
        "!pip install scikit-optimize"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting scikit-optimize\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f4/44/60f82c97d1caa98752c7da2c1681cab5c7a390a0fdd3a55fac672b321cac/scikit_optimize-0.5.2-py2.py3-none-any.whl (74kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 5.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.6/dist-packages (from scikit-optimize) (0.21.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from scikit-optimize) (1.16.5)\n",
            "Requirement already satisfied: scipy>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from scikit-optimize) (1.3.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.19.1->scikit-optimize) (0.14.0)\n",
            "Installing collected packages: scikit-optimize\n",
            "Successfully installed scikit-optimize-0.5.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2x8DHDnzGJMG",
        "colab_type": "code",
        "outputId": "36b0821c-d420-4928-edce-65f5de4715ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "#imports we know we'll need\n",
        "import skopt\n",
        "from skopt import gbrt_minimize, gp_minimize\n",
        "from skopt.utils import use_named_args\n",
        "from skopt.space import Real, Categorical, Integer\n",
        "from keras.optimizers import Adamax"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
            "  warnings.warn(msg, category=DeprecationWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5reBG7yGnAC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dim_learning_rate = Real(low=1e-4, high=1e-2, prior='log-uniform',\n",
        "                         name='learning_rate')\n",
        "dim_no_epoch = Integer(low=20, high=80, name='no_epoch')\n",
        "dim_embedding_size = Integer(low=16, high=256, name='embedding_size')\n",
        "dim_lstm_output_size = Integer(low=16, high=256, name='lstm_output_size')\n",
        "dim_pool_length =Integer(low=2,high=5,name='pool_length')\n",
        "dim_batch_size = Integer(low=1, high=128, name='batch_size')\n",
        "dim_adam_decay = Real(low=1e-6,high=1e-2,name=\"adam_decay\")\n",
        "dim_no_filter = Integer(low=16,high=256,name='no_filter')\n",
        "\n",
        "dimensions = [dim_learning_rate,\n",
        "              dim_batch_size,\n",
        "              dim_adam_decay,\n",
        "              dim_embedding_size,\n",
        "              dim_lstm_output_size,\n",
        "              dim_no_epoch,\n",
        "              dim_no_filter,\n",
        "             ]\n",
        "default_parameters = [1e-3,32,1e-3, 128,128,50,128]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8CGt78Ni96YK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@use_named_args(dimensions=dimensions)\n",
        "def fitness(learning_rate,batch_size,adam_decay,embedding_size,lstm_output_size,no_epoch,no_filter):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(MAX_FEATURES, embedding_size, input_length=MAXLEN))\n",
        "    model.add(Convolution1D(nb_filter=no_filter,filter_length=filter_length,border_mode='valid',activation='relu',subsample_length=1))\n",
        "    model.add(MaxPooling1D(pool_length=pool_length))\n",
        "    model.add(LSTM(lstm_output_size, dropout_W=0.2, dropout_U=0.2, return_sequences=True))\n",
        "    model.add(LSTM(lstm_output_size, dropout_W=0.2, dropout_U=0.2, return_sequences=False))\n",
        "    model.add(Dense(numclasses))\n",
        "    model.add(Activation('softmax'))\n",
        "    \n",
        "    admax=Adamax(lr=learning_rate,decay=adam_decay)\n",
        "    \n",
        "    model.compile(loss='categorical_crossentropy',optimizer=admax,metrics=['accuracy'])\n",
        "    \n",
        "    X_train1,X_valid,y_train1,y_valid=train_test_split(X_train,y_train,test_size=0.2,random_state=42)\n",
        "    bb=model.fit(X_train1, y_train1, batch_size=batch_size, shuffle=True, nb_epoch=no_epoch,validation_data=(X_valid, y_valid))\n",
        "    \n",
        "    accuracy=bb.history['val_acc'][-1]\n",
        "    # Print the classification accuracy.\n",
        "    print()\n",
        "    print(\"Accuracy: {0:.2%}\".format(accuracy))\n",
        "    print()\n",
        "\n",
        "\n",
        "    # Delete the Keras model with these hyper-parameters from memory.\n",
        "    del model\n",
        "    \n",
        "    # Clear the Keras session, otherwise it will keep adding new\n",
        "    # models to the same TensorFlow graph each time we create\n",
        "    # a model with a different set of hyper-parameters.\n",
        "    K.clear_session()\n",
        "    tf.reset_default_graph()\n",
        "    \n",
        "    return -accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "shGQLvmb9rbc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "K.clear_session()\n",
        "tf.reset_default_graph()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BTTq3DNMJrA3",
        "colab_type": "code",
        "outputId": "bd0439e9-72b2-4a4b-ae70-1de1ba89e2b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "gp_result = gp_minimize(func=fitness,\n",
        "                            dimensions=dimensions,\n",
        "                            n_calls=12,\n",
        "                            noise= 0.01,\n",
        "                            n_jobs=-1,\n",
        "                            kappa = 5,\n",
        "                            x0=default_parameters)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(activation=\"relu\", filters=128, kernel_size=3, strides=1, padding=\"valid\")`\n",
            "  \"\"\"\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: UserWarning: Update your `MaxPooling1D` call to the Keras 2 API: `MaxPooling1D(pool_size=3)`\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(128, return_sequences=True, dropout=0.2, recurrent_dropout=0.2)`\n",
            "  import sys\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(128, return_sequences=False, dropout=0.2, recurrent_dropout=0.2)`\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:17: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 2482 samples, validate on 621 samples\n",
            "Epoch 1/50\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "2482/2482 [==============================] - 34s 14ms/step - loss: 0.9921 - acc: 0.5032 - val_loss: 1.0013 - val_acc: 0.4815\n",
            "Epoch 2/50\n",
            "2482/2482 [==============================] - 26s 11ms/step - loss: 0.9642 - acc: 0.5085 - val_loss: 0.9877 - val_acc: 0.4895\n",
            "Epoch 3/50\n",
            "2482/2482 [==============================] - 26s 11ms/step - loss: 0.9528 - acc: 0.5383 - val_loss: 0.9674 - val_acc: 0.5072\n",
            "Epoch 4/50\n",
            "2482/2482 [==============================] - 26s 11ms/step - loss: 0.9344 - acc: 0.5403 - val_loss: 0.9503 - val_acc: 0.5121\n",
            "Epoch 5/50\n",
            "2482/2482 [==============================] - 27s 11ms/step - loss: 0.9132 - acc: 0.5564 - val_loss: 0.9270 - val_acc: 0.5475\n",
            "Epoch 6/50\n",
            "2482/2482 [==============================] - 26s 11ms/step - loss: 0.8931 - acc: 0.5838 - val_loss: 0.8990 - val_acc: 0.5717\n",
            "Epoch 7/50\n",
            "2482/2482 [==============================] - 26s 11ms/step - loss: 0.8746 - acc: 0.5854 - val_loss: 0.8938 - val_acc: 0.5668\n",
            "Epoch 8/50\n",
            "2482/2482 [==============================] - 26s 11ms/step - loss: 0.8558 - acc: 0.6015 - val_loss: 0.8926 - val_acc: 0.5668\n",
            "Epoch 9/50\n",
            "2482/2482 [==============================] - 26s 11ms/step - loss: 0.8506 - acc: 0.6027 - val_loss: 0.8961 - val_acc: 0.5749\n",
            "Epoch 10/50\n",
            "2482/2482 [==============================] - 27s 11ms/step - loss: 0.8514 - acc: 0.6011 - val_loss: 0.8815 - val_acc: 0.5813\n",
            "Epoch 11/50\n",
            "2482/2482 [==============================] - 27s 11ms/step - loss: 0.8337 - acc: 0.6124 - val_loss: 0.8860 - val_acc: 0.5813\n",
            "Epoch 12/50\n",
            "2482/2482 [==============================] - 26s 11ms/step - loss: 0.8284 - acc: 0.6172 - val_loss: 0.8793 - val_acc: 0.5910\n",
            "Epoch 13/50\n",
            "2482/2482 [==============================] - 26s 11ms/step - loss: 0.8225 - acc: 0.6172 - val_loss: 0.8636 - val_acc: 0.5829\n",
            "Epoch 14/50\n",
            "2482/2482 [==============================] - 26s 11ms/step - loss: 0.8143 - acc: 0.6326 - val_loss: 0.8580 - val_acc: 0.5942\n",
            "Epoch 15/50\n",
            "2482/2482 [==============================] - 27s 11ms/step - loss: 0.8153 - acc: 0.6148 - val_loss: 0.8576 - val_acc: 0.5862\n",
            "Epoch 16/50\n",
            "2482/2482 [==============================] - 26s 11ms/step - loss: 0.8078 - acc: 0.6285 - val_loss: 0.8540 - val_acc: 0.5894\n",
            "Epoch 17/50\n",
            "2482/2482 [==============================] - 26s 11ms/step - loss: 0.8080 - acc: 0.6305 - val_loss: 0.8481 - val_acc: 0.6023\n",
            "Epoch 18/50\n",
            "2482/2482 [==============================] - 27s 11ms/step - loss: 0.7944 - acc: 0.6390 - val_loss: 0.8466 - val_acc: 0.6006\n",
            "Epoch 19/50\n",
            "2482/2482 [==============================] - 26s 11ms/step - loss: 0.7956 - acc: 0.6350 - val_loss: 0.8494 - val_acc: 0.5829\n",
            "Epoch 20/50\n",
            "2482/2482 [==============================] - 26s 11ms/step - loss: 0.7848 - acc: 0.6289 - val_loss: 0.8460 - val_acc: 0.6071\n",
            "Epoch 21/50\n",
            "2482/2482 [==============================] - 26s 11ms/step - loss: 0.7944 - acc: 0.6362 - val_loss: 0.8493 - val_acc: 0.6023\n",
            "Epoch 22/50\n",
            "2482/2482 [==============================] - 26s 11ms/step - loss: 0.7837 - acc: 0.6495 - val_loss: 0.8534 - val_acc: 0.6039\n",
            "Epoch 23/50\n",
            "2482/2482 [==============================] - 26s 11ms/step - loss: 0.7882 - acc: 0.6378 - val_loss: 0.8370 - val_acc: 0.5958\n",
            "Epoch 24/50\n",
            "2482/2482 [==============================] - 26s 11ms/step - loss: 0.7812 - acc: 0.6430 - val_loss: 0.8400 - val_acc: 0.6006\n",
            "Epoch 25/50\n",
            "2482/2482 [==============================] - 26s 10ms/step - loss: 0.7675 - acc: 0.6527 - val_loss: 0.8533 - val_acc: 0.5958\n",
            "Epoch 26/50\n",
            "2482/2482 [==============================] - 26s 10ms/step - loss: 0.7760 - acc: 0.6511 - val_loss: 0.8446 - val_acc: 0.6023\n",
            "Epoch 27/50\n",
            "2482/2482 [==============================] - 26s 11ms/step - loss: 0.7584 - acc: 0.6523 - val_loss: 0.8347 - val_acc: 0.5942\n",
            "Epoch 28/50\n",
            "2482/2482 [==============================] - 26s 11ms/step - loss: 0.7602 - acc: 0.6656 - val_loss: 0.8531 - val_acc: 0.6006\n",
            "Epoch 29/50\n",
            "2482/2482 [==============================] - 26s 10ms/step - loss: 0.7586 - acc: 0.6471 - val_loss: 0.8373 - val_acc: 0.5958\n",
            "Epoch 30/50\n",
            "2482/2482 [==============================] - 26s 10ms/step - loss: 0.7574 - acc: 0.6632 - val_loss: 0.8334 - val_acc: 0.6023\n",
            "Epoch 31/50\n",
            "2482/2482 [==============================] - 26s 10ms/step - loss: 0.7463 - acc: 0.6692 - val_loss: 0.8201 - val_acc: 0.6055\n",
            "Epoch 32/50\n",
            "2482/2482 [==============================] - 26s 10ms/step - loss: 0.7373 - acc: 0.6628 - val_loss: 0.8288 - val_acc: 0.6200\n",
            "Epoch 33/50\n",
            "2482/2482 [==============================] - 26s 11ms/step - loss: 0.7394 - acc: 0.6664 - val_loss: 0.8314 - val_acc: 0.6071\n",
            "Epoch 34/50\n",
            "2482/2482 [==============================] - 26s 11ms/step - loss: 0.7474 - acc: 0.6624 - val_loss: 0.8256 - val_acc: 0.5974\n",
            "Epoch 35/50\n",
            "2482/2482 [==============================] - 26s 11ms/step - loss: 0.7470 - acc: 0.6612 - val_loss: 0.8386 - val_acc: 0.6087\n",
            "Epoch 36/50\n",
            "2482/2482 [==============================] - 26s 11ms/step - loss: 0.7450 - acc: 0.6720 - val_loss: 0.8168 - val_acc: 0.5990\n",
            "Epoch 37/50\n",
            "2482/2482 [==============================] - 27s 11ms/step - loss: 0.7412 - acc: 0.6720 - val_loss: 0.8319 - val_acc: 0.6071\n",
            "Epoch 38/50\n",
            "2482/2482 [==============================] - 26s 11ms/step - loss: 0.7254 - acc: 0.6809 - val_loss: 0.8454 - val_acc: 0.5958\n",
            "Epoch 39/50\n",
            "2482/2482 [==============================] - 26s 11ms/step - loss: 0.7383 - acc: 0.6757 - val_loss: 0.8312 - val_acc: 0.6055\n",
            "Epoch 40/50\n",
            "2482/2482 [==============================] - 26s 11ms/step - loss: 0.7316 - acc: 0.6849 - val_loss: 0.8163 - val_acc: 0.6103\n",
            "Epoch 41/50\n",
            "2482/2482 [==============================] - 26s 11ms/step - loss: 0.7251 - acc: 0.6829 - val_loss: 0.8165 - val_acc: 0.6087\n",
            "Epoch 42/50\n",
            "2482/2482 [==============================] - 26s 11ms/step - loss: 0.7214 - acc: 0.6829 - val_loss: 0.8113 - val_acc: 0.6216\n",
            "Epoch 43/50\n",
            "2482/2482 [==============================] - 26s 11ms/step - loss: 0.7177 - acc: 0.6906 - val_loss: 0.8199 - val_acc: 0.6103\n",
            "Epoch 44/50\n",
            "2482/2482 [==============================] - 26s 11ms/step - loss: 0.7085 - acc: 0.6894 - val_loss: 0.8508 - val_acc: 0.5974\n",
            "Epoch 45/50\n",
            "2482/2482 [==============================] - 26s 11ms/step - loss: 0.7202 - acc: 0.6841 - val_loss: 0.8124 - val_acc: 0.6167\n",
            "Epoch 46/50\n",
            "2482/2482 [==============================] - 26s 11ms/step - loss: 0.7052 - acc: 0.6926 - val_loss: 0.8124 - val_acc: 0.6167\n",
            "Epoch 47/50\n",
            "2482/2482 [==============================] - 26s 11ms/step - loss: 0.7067 - acc: 0.6958 - val_loss: 0.8163 - val_acc: 0.6151\n",
            "Epoch 48/50\n",
            "2482/2482 [==============================] - 26s 11ms/step - loss: 0.7055 - acc: 0.6922 - val_loss: 0.8285 - val_acc: 0.6184\n",
            "Epoch 49/50\n",
            "2482/2482 [==============================] - 26s 10ms/step - loss: 0.7067 - acc: 0.6865 - val_loss: 0.8166 - val_acc: 0.6151\n",
            "Epoch 50/50\n",
            "2482/2482 [==============================] - 26s 11ms/step - loss: 0.7084 - acc: 0.6849 - val_loss: 0.8185 - val_acc: 0.6167\n",
            "\n",
            "Accuracy: 61.67%\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(activation=\"relu\", filters=226, kernel_size=3, strides=1, padding=\"valid\")`\n",
            "  \"\"\"\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: UserWarning: Update your `MaxPooling1D` call to the Keras 2 API: `MaxPooling1D(pool_size=3)`\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(129, return_sequences=True, dropout=0.2, recurrent_dropout=0.2)`\n",
            "  import sys\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(129, return_sequences=False, dropout=0.2, recurrent_dropout=0.2)`\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:17: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 2482 samples, validate on 621 samples\n",
            "Epoch 1/26\n",
            "2482/2482 [==============================] - 10s 4ms/step - loss: 1.0413 - acc: 0.5008 - val_loss: 1.0188 - val_acc: 0.4815\n",
            "Epoch 2/26\n",
            "2482/2482 [==============================] - 7s 3ms/step - loss: 0.9926 - acc: 0.5089 - val_loss: 1.0066 - val_acc: 0.4815\n",
            "Epoch 3/26\n",
            "2482/2482 [==============================] - 7s 3ms/step - loss: 0.9828 - acc: 0.5089 - val_loss: 1.0009 - val_acc: 0.4815\n",
            "Epoch 4/26\n",
            "2482/2482 [==============================] - 7s 3ms/step - loss: 0.9749 - acc: 0.5089 - val_loss: 0.9978 - val_acc: 0.4815\n",
            "Epoch 5/26\n",
            "2482/2482 [==============================] - 7s 3ms/step - loss: 0.9704 - acc: 0.5089 - val_loss: 0.9966 - val_acc: 0.4815\n",
            "Epoch 6/26\n",
            "2482/2482 [==============================] - 7s 3ms/step - loss: 0.9673 - acc: 0.5097 - val_loss: 0.9921 - val_acc: 0.4815\n",
            "Epoch 7/26\n",
            "2482/2482 [==============================] - 8s 3ms/step - loss: 0.9661 - acc: 0.5101 - val_loss: 0.9936 - val_acc: 0.4815\n",
            "Epoch 8/26\n",
            "2482/2482 [==============================] - 7s 3ms/step - loss: 0.9666 - acc: 0.5101 - val_loss: 0.9913 - val_acc: 0.4831\n",
            "Epoch 9/26\n",
            "2482/2482 [==============================] - 7s 3ms/step - loss: 0.9675 - acc: 0.5205 - val_loss: 0.9910 - val_acc: 0.4847\n",
            "Epoch 10/26\n",
            "2482/2482 [==============================] - 7s 3ms/step - loss: 0.9631 - acc: 0.5141 - val_loss: 0.9922 - val_acc: 0.4847\n",
            "Epoch 11/26\n",
            "2482/2482 [==============================] - 7s 3ms/step - loss: 0.9640 - acc: 0.5181 - val_loss: 0.9909 - val_acc: 0.4863\n",
            "Epoch 12/26\n",
            "2482/2482 [==============================] - 7s 3ms/step - loss: 0.9599 - acc: 0.5193 - val_loss: 0.9905 - val_acc: 0.4879\n",
            "Epoch 13/26\n",
            "2482/2482 [==============================] - 7s 3ms/step - loss: 0.9583 - acc: 0.5230 - val_loss: 0.9902 - val_acc: 0.4863\n",
            "Epoch 14/26\n",
            "2482/2482 [==============================] - 8s 3ms/step - loss: 0.9581 - acc: 0.5262 - val_loss: 0.9906 - val_acc: 0.4879\n",
            "Epoch 15/26\n",
            "2482/2482 [==============================] - 7s 3ms/step - loss: 0.9611 - acc: 0.5302 - val_loss: 0.9884 - val_acc: 0.4895\n",
            "Epoch 16/26\n",
            "2482/2482 [==============================] - 7s 3ms/step - loss: 0.9590 - acc: 0.5222 - val_loss: 0.9887 - val_acc: 0.4895\n",
            "Epoch 17/26\n",
            "2482/2482 [==============================] - 7s 3ms/step - loss: 0.9594 - acc: 0.5262 - val_loss: 0.9894 - val_acc: 0.4879\n",
            "Epoch 18/26\n",
            "2482/2482 [==============================] - 7s 3ms/step - loss: 0.9565 - acc: 0.5330 - val_loss: 0.9872 - val_acc: 0.4879\n",
            "Epoch 19/26\n",
            "2482/2482 [==============================] - 7s 3ms/step - loss: 0.9583 - acc: 0.5346 - val_loss: 0.9883 - val_acc: 0.4879\n",
            "Epoch 20/26\n",
            "2482/2482 [==============================] - 7s 3ms/step - loss: 0.9572 - acc: 0.5282 - val_loss: 0.9870 - val_acc: 0.4879\n",
            "Epoch 21/26\n",
            "2482/2482 [==============================] - 7s 3ms/step - loss: 0.9590 - acc: 0.5310 - val_loss: 0.9864 - val_acc: 0.4879\n",
            "Epoch 22/26\n",
            "2482/2482 [==============================] - 7s 3ms/step - loss: 0.9581 - acc: 0.5302 - val_loss: 0.9861 - val_acc: 0.4879\n",
            "Epoch 23/26\n",
            "2482/2482 [==============================] - 7s 3ms/step - loss: 0.9525 - acc: 0.5351 - val_loss: 0.9861 - val_acc: 0.4879\n",
            "Epoch 24/26\n",
            "2482/2482 [==============================] - 7s 3ms/step - loss: 0.9542 - acc: 0.5334 - val_loss: 0.9848 - val_acc: 0.4911\n",
            "Epoch 25/26\n",
            "2482/2482 [==============================] - 8s 3ms/step - loss: 0.9537 - acc: 0.5395 - val_loss: 0.9851 - val_acc: 0.4960\n",
            "Epoch 26/26\n",
            "2482/2482 [==============================] - 7s 3ms/step - loss: 0.9531 - acc: 0.5334 - val_loss: 0.9858 - val_acc: 0.4944\n",
            "\n",
            "Accuracy: 49.44%\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(activation=\"relu\", filters=142, kernel_size=3, strides=1, padding=\"valid\")`\n",
            "  \"\"\"\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: UserWarning: Update your `MaxPooling1D` call to the Keras 2 API: `MaxPooling1D(pool_size=3)`\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(152, return_sequences=True, dropout=0.2, recurrent_dropout=0.2)`\n",
            "  import sys\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(152, return_sequences=False, dropout=0.2, recurrent_dropout=0.2)`\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:17: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 2482 samples, validate on 621 samples\n",
            "Epoch 1/39\n",
            "2482/2482 [==============================] - 12s 5ms/step - loss: 0.9983 - acc: 0.5012 - val_loss: 0.9894 - val_acc: 0.4960\n",
            "Epoch 2/39\n",
            "2482/2482 [==============================] - 10s 4ms/step - loss: 0.9624 - acc: 0.5165 - val_loss: 0.9881 - val_acc: 0.4944\n",
            "Epoch 3/39\n",
            "2482/2482 [==============================] - 9s 4ms/step - loss: 0.9508 - acc: 0.5254 - val_loss: 0.9712 - val_acc: 0.5105\n",
            "Epoch 4/39\n",
            "2482/2482 [==============================] - 9s 4ms/step - loss: 0.9280 - acc: 0.5528 - val_loss: 0.9535 - val_acc: 0.5072\n",
            "Epoch 5/39\n",
            "2482/2482 [==============================] - 9s 4ms/step - loss: 0.9063 - acc: 0.5681 - val_loss: 0.9447 - val_acc: 0.5282\n",
            "Epoch 6/39\n",
            "2482/2482 [==============================] - 10s 4ms/step - loss: 0.8904 - acc: 0.5782 - val_loss: 0.9074 - val_acc: 0.5523\n",
            "Epoch 7/39\n",
            "2482/2482 [==============================] - 9s 4ms/step - loss: 0.8636 - acc: 0.6019 - val_loss: 0.9064 - val_acc: 0.5475\n",
            "Epoch 8/39\n",
            "2482/2482 [==============================] - 9s 4ms/step - loss: 0.8618 - acc: 0.5979 - val_loss: 0.8919 - val_acc: 0.5700\n",
            "Epoch 9/39\n",
            "2482/2482 [==============================] - 9s 4ms/step - loss: 0.8522 - acc: 0.5979 - val_loss: 0.8774 - val_acc: 0.5668\n",
            "Epoch 10/39\n",
            "2482/2482 [==============================] - 9s 4ms/step - loss: 0.8395 - acc: 0.6152 - val_loss: 0.8707 - val_acc: 0.5813\n",
            "Epoch 11/39\n",
            "2482/2482 [==============================] - 9s 4ms/step - loss: 0.8348 - acc: 0.6185 - val_loss: 0.8637 - val_acc: 0.5829\n",
            "Epoch 12/39\n",
            "2482/2482 [==============================] - 9s 4ms/step - loss: 0.8263 - acc: 0.6245 - val_loss: 0.8808 - val_acc: 0.5700\n",
            "Epoch 13/39\n",
            "2482/2482 [==============================] - 9s 4ms/step - loss: 0.8224 - acc: 0.6293 - val_loss: 0.8626 - val_acc: 0.5862\n",
            "Epoch 14/39\n",
            "2482/2482 [==============================] - 9s 4ms/step - loss: 0.8161 - acc: 0.6237 - val_loss: 0.8621 - val_acc: 0.5733\n",
            "Epoch 15/39\n",
            "2482/2482 [==============================] - 9s 4ms/step - loss: 0.8164 - acc: 0.6297 - val_loss: 0.8599 - val_acc: 0.5813\n",
            "Epoch 16/39\n",
            "2482/2482 [==============================] - 10s 4ms/step - loss: 0.8069 - acc: 0.6261 - val_loss: 0.8511 - val_acc: 0.5894\n",
            "Epoch 17/39\n",
            "2482/2482 [==============================] - 9s 4ms/step - loss: 0.7918 - acc: 0.6499 - val_loss: 0.8670 - val_acc: 0.5829\n",
            "Epoch 18/39\n",
            "2482/2482 [==============================] - 9s 4ms/step - loss: 0.7891 - acc: 0.6495 - val_loss: 0.8607 - val_acc: 0.5910\n",
            "Epoch 19/39\n",
            "2482/2482 [==============================] - 9s 4ms/step - loss: 0.7971 - acc: 0.6463 - val_loss: 0.8602 - val_acc: 0.5910\n",
            "Epoch 20/39\n",
            "2482/2482 [==============================] - 9s 4ms/step - loss: 0.7878 - acc: 0.6467 - val_loss: 0.8469 - val_acc: 0.5990\n",
            "Epoch 21/39\n",
            "2482/2482 [==============================] - 9s 4ms/step - loss: 0.7727 - acc: 0.6519 - val_loss: 0.8409 - val_acc: 0.5958\n",
            "Epoch 22/39\n",
            "2482/2482 [==============================] - 9s 4ms/step - loss: 0.7729 - acc: 0.6567 - val_loss: 0.8423 - val_acc: 0.5942\n",
            "Epoch 23/39\n",
            "2482/2482 [==============================] - 9s 4ms/step - loss: 0.7713 - acc: 0.6555 - val_loss: 0.8458 - val_acc: 0.5974\n",
            "Epoch 24/39\n",
            "2482/2482 [==============================] - 9s 4ms/step - loss: 0.7684 - acc: 0.6579 - val_loss: 0.8471 - val_acc: 0.5894\n",
            "Epoch 25/39\n",
            "2482/2482 [==============================] - 9s 4ms/step - loss: 0.7804 - acc: 0.6446 - val_loss: 0.8363 - val_acc: 0.6055\n",
            "Epoch 26/39\n",
            "2482/2482 [==============================] - 9s 4ms/step - loss: 0.7632 - acc: 0.6543 - val_loss: 0.8506 - val_acc: 0.6023\n",
            "Epoch 27/39\n",
            "2482/2482 [==============================] - 9s 4ms/step - loss: 0.7614 - acc: 0.6668 - val_loss: 0.8397 - val_acc: 0.5942\n",
            "Epoch 28/39\n",
            "2482/2482 [==============================] - 9s 4ms/step - loss: 0.7509 - acc: 0.6688 - val_loss: 0.8327 - val_acc: 0.6103\n",
            "Epoch 29/39\n",
            "2482/2482 [==============================] - 9s 4ms/step - loss: 0.7554 - acc: 0.6648 - val_loss: 0.8402 - val_acc: 0.6055\n",
            "Epoch 30/39\n",
            "2482/2482 [==============================] - 9s 4ms/step - loss: 0.7519 - acc: 0.6648 - val_loss: 0.8336 - val_acc: 0.6087\n",
            "Epoch 31/39\n",
            "2482/2482 [==============================] - 9s 4ms/step - loss: 0.7456 - acc: 0.6793 - val_loss: 0.8266 - val_acc: 0.6103\n",
            "Epoch 32/39\n",
            "2482/2482 [==============================] - 10s 4ms/step - loss: 0.7450 - acc: 0.6700 - val_loss: 0.8389 - val_acc: 0.6216\n",
            "Epoch 33/39\n",
            "2482/2482 [==============================] - 9s 4ms/step - loss: 0.7403 - acc: 0.6708 - val_loss: 0.8343 - val_acc: 0.6167\n",
            "Epoch 34/39\n",
            "2482/2482 [==============================] - 9s 4ms/step - loss: 0.7386 - acc: 0.6720 - val_loss: 0.8438 - val_acc: 0.6167\n",
            "Epoch 35/39\n",
            "2482/2482 [==============================] - 9s 4ms/step - loss: 0.7392 - acc: 0.6757 - val_loss: 0.8266 - val_acc: 0.6103\n",
            "Epoch 36/39\n",
            "2482/2482 [==============================] - 9s 4ms/step - loss: 0.7347 - acc: 0.6781 - val_loss: 0.8223 - val_acc: 0.6087\n",
            "Epoch 37/39\n",
            "2482/2482 [==============================] - 9s 4ms/step - loss: 0.7438 - acc: 0.6749 - val_loss: 0.8240 - val_acc: 0.6119\n",
            "Epoch 38/39\n",
            "2482/2482 [==============================] - 10s 4ms/step - loss: 0.7405 - acc: 0.6805 - val_loss: 0.8271 - val_acc: 0.6151\n",
            "Epoch 39/39\n",
            "2482/2482 [==============================] - 9s 4ms/step - loss: 0.7356 - acc: 0.6724 - val_loss: 0.8257 - val_acc: 0.6151\n",
            "\n",
            "Accuracy: 61.51%\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(activation=\"relu\", filters=94, kernel_size=3, strides=1, padding=\"valid\")`\n",
            "  \"\"\"\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: UserWarning: Update your `MaxPooling1D` call to the Keras 2 API: `MaxPooling1D(pool_size=3)`\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(244, return_sequences=True, dropout=0.2, recurrent_dropout=0.2)`\n",
            "  import sys\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(244, return_sequences=False, dropout=0.2, recurrent_dropout=0.2)`\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:17: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 2482 samples, validate on 621 samples\n",
            "Epoch 1/40\n",
            "2482/2482 [==============================] - 64s 26ms/step - loss: 0.9869 - acc: 0.5073 - val_loss: 0.9915 - val_acc: 0.4815\n",
            "Epoch 2/40\n",
            "2482/2482 [==============================] - 61s 25ms/step - loss: 0.9671 - acc: 0.5121 - val_loss: 0.9846 - val_acc: 0.4911\n",
            "Epoch 3/40\n",
            "2482/2482 [==============================] - 62s 25ms/step - loss: 0.9606 - acc: 0.5250 - val_loss: 0.9978 - val_acc: 0.4944\n",
            "Epoch 4/40\n",
            "2482/2482 [==============================] - 61s 25ms/step - loss: 0.9490 - acc: 0.5379 - val_loss: 0.9750 - val_acc: 0.4992\n",
            "Epoch 5/40\n",
            "2482/2482 [==============================] - 62s 25ms/step - loss: 0.9440 - acc: 0.5443 - val_loss: 0.9649 - val_acc: 0.4767\n",
            "Epoch 6/40\n",
            "2482/2482 [==============================] - 61s 25ms/step - loss: 0.9298 - acc: 0.5564 - val_loss: 0.9669 - val_acc: 0.5056\n",
            "Epoch 7/40\n",
            "2482/2482 [==============================] - 61s 25ms/step - loss: 0.9196 - acc: 0.5564 - val_loss: 0.9489 - val_acc: 0.5105\n",
            "Epoch 8/40\n",
            "2482/2482 [==============================] - 61s 25ms/step - loss: 0.9022 - acc: 0.5649 - val_loss: 0.9361 - val_acc: 0.5024\n",
            "Epoch 9/40\n",
            "2482/2482 [==============================] - 62s 25ms/step - loss: 0.8950 - acc: 0.5749 - val_loss: 0.9207 - val_acc: 0.5443\n",
            "Epoch 10/40\n",
            "2482/2482 [==============================] - 61s 25ms/step - loss: 0.8868 - acc: 0.5689 - val_loss: 0.9181 - val_acc: 0.5475\n",
            "Epoch 11/40\n",
            "2482/2482 [==============================] - 60s 24ms/step - loss: 0.8772 - acc: 0.5919 - val_loss: 0.9199 - val_acc: 0.5588\n",
            "Epoch 12/40\n",
            "2482/2482 [==============================] - 61s 25ms/step - loss: 0.8783 - acc: 0.5749 - val_loss: 0.9059 - val_acc: 0.5556\n",
            "Epoch 13/40\n",
            "2482/2482 [==============================] - 62s 25ms/step - loss: 0.8744 - acc: 0.5778 - val_loss: 0.9079 - val_acc: 0.5604\n",
            "Epoch 14/40\n",
            "2482/2482 [==============================] - 63s 26ms/step - loss: 0.8714 - acc: 0.5874 - val_loss: 0.8989 - val_acc: 0.5572\n",
            "Epoch 15/40\n",
            "2282/2482 [==========================>...] - ETA: 4s - loss: 0.8680 - acc: 0.5868"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ccJ4et_UGAa9",
        "colab_type": "code",
        "outputId": "ef318660-ff6e-4c5f-b251-b9695fb5e183",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 188
        }
      },
      "source": [
        "!pip install scikit-optimize"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting scikit-optimize\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f4/44/60f82c97d1caa98752c7da2c1681cab5c7a390a0fdd3a55fac672b321cac/scikit_optimize-0.5.2-py2.py3-none-any.whl (74kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 6.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from scikit-optimize) (1.16.4)\n",
            "Requirement already satisfied: scipy>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from scikit-optimize) (1.3.0)\n",
            "Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.6/dist-packages (from scikit-optimize) (0.21.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.19.1->scikit-optimize) (0.13.2)\n",
            "Installing collected packages: scikit-optimize\n",
            "Successfully installed scikit-optimize-0.5.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8bHYqnWj8nQB",
        "colab_type": "code",
        "outputId": "579eb83a-1a3d-4e24-f825-c86ac872cc92",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(history.history['val_loss'],'r')\n",
        "plt.plot(history.history['val_acc'],'r')\n",
        "plt.plot(history.history['loss'],'b')\n",
        "plt.plot(history.history['acc'],'b')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f6a1f47cd30>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJztnXd4FOUTx79vEjpIB0F6VUClSRUQ\nkCIiiAgKiIgoNqzYEEVBEUWUIv4QBUQ6ijSRJlVBKaGIFEGk994DJLn5/fG95UouySW5y5XM53nu\nSXb33d25u73Z2Zl5Z4yIQFEURQkvIgItgKIoiuJ7VLkriqKEIarcFUVRwhBV7oqiKGGIKndFUZQw\nRJW7oihKGKLKXVEUJQxR5a4oihKGqHJXFEUJQ6ICdeICBQpIqVKlAnV6RVGUkGTDhg2nRKRgcuMC\nptxLlSqF6OjoQJ1eURQlJDHG7PdmnLplFEVRwhBV7oqiKGGIKndFUZQwRJW7oihKGKLKXVEUJQxR\n5a4oihKGqHJXFEUJQ0JOuf/5J/D224B2B1QURUmckFPuGzcCn34K7N0baEkURVGCl2SVuzFmnDHm\nhDFmayLbjTFmhDFmtzFmizGmuu/FdNCkCf8uW+bPsyiKooQ23lju4wG0TGL7fQDK2189AYxKu1iJ\nc+utwM03q3JXFEVJimSVu4j8BuBMEkPaApggZA2APMaYIr4S0B1jaL0vW6Z+d0VRlMTwhc/9FgAH\nnZYP2df5h3Xr0GTfOBw/Dmzf7rezKIqihDTpGlA1xvQ0xkQbY6JPnjyZuoNs3Iimaz8GACz76A/A\nZvOhhIqiKOGBL5T7YQDFnZaL2dclQES+EZGaIlKzYMFkyxF75tlnUWrLXJTOegTLph0H7r4b+Pvv\n1B1LURQlTPGFcp8L4HF71kwdAOdF5KgPjps4lSqhSeciWJG9FeJ3/Qc0aAAc9ng/URRFyZB4kwo5\nFcCfACoaYw4ZY3oYY541xjxrHzIfwB4AuwF8C+B5v0nrRJOmBueuZMHmMdHA9evAc89phFVRFMVO\nsp2YRKRTMtsFwAs+k8hLGjfm32X/FkeNDz8EXn8dmDYN6JSkuIqiKBmCkJuhalGkCHDbbfZ891de\nAWrVAl56CUhtoFZRFCWMCFnlDjDf/bffgOvxkcC4ccD581TwiqIoGZyQVu5NmwJXrgDr1gGoXBl4\n9126ZiZPVv+7oigZmpBW7o0accbqjVIEb78NVK0KPPYYUL480K8fsHNnQGVUFEUJBCGt3PPlA6pV\nAxYvtq/InJl+mnHjgFKlgI8+YjGaRx8FLl8OpKiKoijpSkgrdwB45BFg9WpgxAj7ily5gO7dgSVL\ngEOHgPfeA374gbnwBw4EVFZFUTIgP/8M/Pdfup825JX7668D7doBr77Kz9CFokWBAQOAefP44d51\nF/DHHwGRU1GUDMjVq0D79sD99zNAmI6EvHKPiAAmTQKqV2eK+6ZNHga1agWsWQPcdBMT5JcsSXc5\nFUXJgPz1FxAby9jfW2+l66lDXrkDQPbswNy59MG3bk1vTAJuuw1YuxYoUQJ4803NplEUxf+sX8+/\nHToAI0c6BQj9T1god4CTmn75Bbh4EbjnHmDzZg+D8uUD+valeT9/fnqLqChKRiM6GihcGPj+exqY\n3bsDZ5Jqj+E7wka5A8DttwOLFgExMUDdusDYsR4M9C5dmEnz4YdqvSuK4l/Wrwdq1gSyZaP/+MQJ\n4Pl0Kb8VXsodoFLftImVgJ96ijdKlyzITJmYD792rfbqUxTFf1y6BOzYwUQOgIHB/v2B6dOBqVP9\nfvqwU+4AUKgQsHAh8P77wIQJQLdubgOeeIKZNB9+GAjxFEXJCGzcSO9AzZqOdW++SauzShW/nz4s\nlTsAREYCH3xA/f3TT3TX3CBLFn7IK1cCv/8eKBEVRQlnoqP511m5R0UB335LH7KfCVvlbvH666xE\n8OKLwLVrThuefpom/sCBAZNNUZQwZv16oHhxBlQDQNgr9yxZOHv133+Bzz932pA9O9C7N036pUsD\nJp+iKGFKdLTD3x4Awl65A0DLlsBDD7HUjEsFguefZ3pShw7Arl0Bk09RFAD//BM+/RjOngV273Z1\nyaQzGUK5A8DQofz76qtOK3PmZHJ8VBSnB586FRDZFCXDI8IGDX36BFoS37BhA/+q5e5/SpRgufeZ\nMxlgvUHp0sCcOcDBgyxS4+KYVxQlXTh0CDh6lNZ7ajh7NrhSm62ZqTVqBEyEDKPcAbrYa9ZkDRoX\nBV+3LmeQrVoF9OgBxMUFTEZFyZBs3Mi/e/Z43i7CAoCxsZ63f/45cO+9nCQUDERHA+XKAXnzBkyE\nDKXcs2QBfv2VCr5jR04Yu8Ejj9ApP3kyUK8e8PffAZNTUTIcVsW/o0c9V0/csAF44AF2WvNEdDRv\nAOvWpV2WvXuBChWAiRO9G3/xYsJYgTUzNYBkKOUOAHnysHZPo0bA448z5fQG77zDi2ffPs4me/99\nddMoSnpgWe4Af3/u7NjBv56Ut4hjf18o9+nTmV7XrZubgkiE7t2BMmUcbqHjx+nmDaC/HciAyh1w\nxFFbtgR69nSqAGwMLfjt29m9acAAoE4dFqtRFMV/bNpENwbg2TVjZbNZvmxnjhxxWM5r16Zdljlz\n2K7TUhAjRyY+9uxZlqSNiQHuuw+YNcvz5KUAkCGVO8A6PjNncn6BlUlzgwIF+Eg2fTrLSw4eHBAZ\nFSVDcPIkA6oPP8xlT12LLOW+eXNCv7tltVeuTMvdZku9LMeO8QbRvj0Vddu2nAE5ZIjn8bNnU555\n8/i0//DDdO8aw+UAkmGVOwBkzQo88wywYEEiXbA6dqQl/8kn9MMpiuJ7LH97s2Zsk5mY5R4VRTfp\ntm2u2zZupDLt2RM4d44uldTyyy9087RpwyDdjz9SD7zxBvDnnwnHT5/OjLsWLegCuPdeNga67Ta6\nCAJIhlbuAK+HiAhg1KhEBgwZwkI1LgnyiqL4DMvyrlaNvmt35S5Chd2yJZfdXTMbNwIVKzJPHkib\na2buXKBkSUftl0yZgHHj2Avi009dx546RYXesSNvLjlysNfnK68Ehb7I8Mr9lls4e3XcuERaHBYr\nxibbc+bQxE+M8+f5aHbsmN9kVZSwZNMm9ljIm9ezcj96lHW7W7bkGMun7bx/9eoOazm1QdUrV5hO\n16YNlbVFjhzACy9QBzjn4c+cCcTH8+neInNm+nmfeip1MvgQr5S7MaalMWanMWa3MeZtD9tLGmOW\nGmO2GGNWGGOK+V5U/9GrF+MiiZZYfuUVpka99JIjeyY+nhfV4MFs/VSgAFO1evRIL7EVJTzYuNHh\nn7aUu3MjHcvfXrEig5TOlvvJk8xMqV6dT9h33ZV6y33JEgZG27RJuK1XL/pxnX3v06ezKmHVqqk7\nn59JVrkbYyIBfAXgPgCVAHQyxlRyGzYEwAQRuQPAAACDfC2oP2nQgOWVR45MpDmTVX1s924+gjVv\nzpzK6tXZ9PbcOfrkevRg+z4rbUtRlKS5cIG/q2rVuFymDHD1qusTsKXcy5encv/7b44BHP56a/9a\ntdiU2tqeEubOBW66CWjYMOG2QoWAJ59kosWRI0x3XLGCVruzlR9EeGO51wKwW0T2iMh1ANMAtHUb\nUwmANfd3uYftQY0xvDFv3uw5ZgKAAZMOHehTO34c6NqVs6AOHuSOH38MDBrEu/sXX6Sr/IoSsljN\njp0td8DVNbNrFw2s4sVpmcfFUYEDrv56AKhdm9krltL3FpuNv+1Wreha8cRrr/HcI0YAM2ZwH2eX\nTJDhjXK/BcBBp+VD9nXO/AXgIfv/7QDkMsbkT7t46UeXLkDu3EmntGLKFFoaf/0F/O9/3KmYkweq\nYEF2eZowQX3viuIN7pZ3Ysq9fHlmPli545bffdMmZqtY0/xr1+bflLpm1q1j6QJPLhmLsmWZ6jhq\nFPDdd0ClSunSUSm1+Cqg+jqARsaYTQAaATgMIN59kDGmpzEm2hgTfTLISnvmzEm9/OOPLBD51ls0\nzPfvdxoUFZV8etOrr9Jy+Oorf4qrKOHBxo3AzTcDRYpwuWRJPkq7K/cKFfh/sWKcnGL53Z399QDb\nZxYrlvKg6pw5/H1bGTmJ8eabNPA2bAhqqx3wTrkfBlDcabmYfd0NROSIiDwkItUA9LWvO+d+IBH5\nRkRqikjNggULpkFs/9C3L/DYY/S0DB1Kz0uVKvTCeE2FCrz7/+9/iaTfKIpyAyvTxSJLFipnS7nH\nxXESiqXcjaH1Hh3NDLXduxNOFqpVK+WW+9y59LUnV+irRg2gaVP+HwbKfT2A8saY0saYzAAeBTDX\neYAxpoAxxjpWHwDjfCtm+lCwIJ+2tmxh5tWyZWxgPi6l76Z3b+DMGWD8eH+IqSjhQUwMS31YLhkL\n53TI/fup4C3lDlC579jBKq5Awv1r1+b+3noHVq+mHO3aeTd+5Eg+mVes6N34AJGscheROAC9ACwC\nsAPADyKyzRgzwBhjOajuAbDTGLMLQGEAId+YNFMmoHFjvkaPZuaj19x9N62HoUNTuKOiZCC2buXv\nw93ydlbuVqaMs3K/6y4GMy2ry31/y+/ujWtGhD7YIkWYDeMNt97KLm5Bjlc+dxGZLyIVRKSsiAy0\nr+snInPt/88QkfL2MU+JSNiUUnzuORoPSc1fSoAx7My9ezeDq4qiJMQ908WiTBmmG8bEeFbuVlB1\nzhz62N0bUNeoweCrN66ZX36h5d6vH/sqhxEZfoZqcjz4IOM9iZYnSIx27ZhA/8wzwdeA++xZrVev\nBJ5NmzhfpFQp1/VWxsy+fVTuefJwkqBF4cJMi/Rk9QNMeqhSJXnlHh/Ptn7lyoXl5ENV7smQKRPw\n9NO03FNUOywqipZFxYq8Q1g9FYOBt99mKWMN+CopZepUtxSyVLJ7N4/VoEHCSUDO6ZBWGqT7GMt6\nT6zy4r33spTAmDGJyzBlCl1DH33EH3qYocrdC6ziYqNHp3DHvHmBRYuA/PlZ6zkt1ep8hQhn0V65\nwhl2iuItR44AnTuzqU1auHKFJXWjooAvv0y43V25O7tkLKxGGO4uHYuBA5nW+PTTwNdfJ9x+7Rpd\nMdWrc3JiGKLK3QuKFWPZmLFjU9GYqWhRtn4SYdmCQPd43L6dtbMBYOHCwMqihBbLl/PvrFnM9U4N\nIsCzz9ItOGUK89rdKViQxbq2bQMOHPCs3Nu2ZTtMT6UCAM4UnzULaN2agTPn2Ynx8Vzet4+zyiPC\nUw2G57vyA88/zwqfM2akYucKFWgtHzvG5Pm0NBNIK4sW8e+dd6YwSqyEJXv2MKjoDcuW0dqOiUnl\nDwF8/J04EfjgA5b08IQxtN4XL+ayJ+VeqRIDofnyJX6uLFmAn36iW/TFF9lbs0IFBk5ff50lgps1\nS937CAFUuXtJ06aMuzzzjGOSXP78rBfmFXfdBQwbxgv2s8/8KmuSLFzI0qg9etDvuXt34GRRAs8b\nb1D5XbqU9DgRJgY88AAVZGqywNavB15+mfVb3n036bFlyzqCXJ6Uu7dkzgz88AOfFi5fZgXHV15h\nhsSMGUFb9MsXRAVagFAhIoKTTidPpvGSKROvvSFDeL0n9nToQs+e/IH07csd6tb1u9wuXLkC/PYb\nH0OsadaLFjl6VyoZi5gY3uzj4mgFJ2ZJA7zY9+/nzaBGDSrnvXtZ18UbRGg9FypEyz05V4jldwcY\nUE0LmTKlIt0t9FHLPQU0a8ZJp2PG8FqZORMoUYLXbFycFwcwht3US5RgA+6zZ/0tsisrVzJo0KIF\nfzBly6prJjEOHGDDhdSUjvU3V68C16+n/TiLFzsypix/emIssxd9bdKErkWAxZfcSczluHw5UxP7\n9k3alWJhKfciRdh6T0kxqtzTQPbsrO67ZYvngLxHcucGpk1j5sHjj7NGRkyMY3tcHA84ZozvFe+i\nRQw0WY8ZLVvyRxeMCizQzJzJCLp7S7dgoEULBgk9ceUKg4jelLydNYs55LVqJZ85tXQpFe2tt9I4\nadyYrhnnBgjffstgqFUWwJmBA7n/E08kLxfgUO5pcclkdEQkIK8aNWpIOGCziTRtKpInj8iJEynY\ncehQEf40RCIiRG67TaRePZHs2R3ro6JE/vzTd8LeeqtIixaO5XnzeJ7Fi313jnDhmWf42Xz9daAl\nceX6dZFMmURKlvS8felSyv3000kfJzZWJF8+kcceE+nbVyQyUuTCBc9jbTaRQoVEunRxrPvuO55n\n9Wouf/+9iDF8lS7teqw//+TYzz/39l2K/POPd+8jAwIgWrzQsWq5pxFjWLv/0iU+cXrNK6+w2t2M\nGcwbLleOfsinnuLj7saNjNx64765do2WWlKP6vv3s/+js1/1nnsYcNKUyIRYvTK3bw+sHO7s2sWS\n0vv3M33LHavO+c8/J52V9dtvLG7Xrh2t8Ph44PffPY/dvp0pvFYDaoB56tmy0XqfPh3o3p1ZB4sW\nMcXw9dcdYwcOpCumZ0/v32epUnwKSO+4VDjhzR3AH69wsdwtXnuNRsv69T486Jo1tN7btaP15MzJ\nkyLjx4s89JBIzpy0curXFzl+3POxRo/mmO3bXdc3ayZSqZL3MsXHi/z8s8jlyyl7L6FG4cL8vO69\n1zfHO3064XeYGqZNczzZLVyYcPvDDzu2J/XU16uXSNasIpcu8bvMnFnk9dc9jx0xgsfbu9d1fZcu\nfNKMjBRp0IDHEhF54w2Onz9fZPNm/j9gQMrf6/XrvvnMwgx4abmrcvcR58+LFCkiUqZM4vo1VQwZ\nwq/pyy+5vHu3SM+e/DECIkWL0oUweLBItmx8XN+yJeFx2rUTKV484Y/l8895nP37vZNn2DCOf/75\nNL2toObMGb5HY/j5ppW9e6lIx4xJ+7EsFwog8tFHCbeXLMkbdmSkyNtvez6GzSZSrJhI27aOdQ0a\niNSs6Xl827a8sN1ZvJhy1K7t6oaJiRGpUoU/iPvuE8mVi5+p4hNUuQeAtWupX2vX9qFhGx8v0qoV\nlXn79vTPZ85Mhb5hg6uyXr+eyihnTpE5cxzbrl8Xuekmz/7L7dt5GYwenbwsW7eKZMnC40dEiPz1\nl2/eY7Bh+Yjvvpt/z55N2/HeeYfHadIk7bK1acMnrQoVRB580HXbiRM8z2efiTRuzDiOJ9at47jx\n4x3r+vXjd3runOvYuDiR3LlFnnoq4XFsNsZt3PcREdm0ibEBIPGbjJIqVLkHiFmzaPC1a8ffhU84\neZKWVs6cfOQ9ciTxsYcO0QIDqNCrVRNp3pzLM2YkHG+ziZQowWDuqVOJH/faNZGqVUUKFuQNIV8+\nKpBwfGy2goVffMG/f/yR+mNdv04XT0QErenTp9MmW+nSIo88ItK5M68JZ+bPp7wrVjiesHbtSniM\nPn0SyrJ8OcfPnes6dv16rp8yJeWyfvEFnxaPHUv5vkqieKvcNaDqYx58kD06Zs1iTOnECfYMmD6d\nk6COHk3FQQsUYFPuQ4eAwYMd/SY9ccstzGf/6iumWhYuzMkm5cqxUp47xrBZwdq1zH3/8ksG7Nzp\n14+d6seO5QzXAQOYRjlrVireUJDzzz+c+HL//VxOS1B1zhz2aezXj0HLefNSf6yLF/ld3n47JxId\nOuTaiH39en6f1auz9op1fndmzeJUfOd88zp1OF3fPSXSOb89pbz6KoOr7vXWlfTBmzuAP17harlb\nvPyy3IhrOb+yZRPp3TuFaZPpwd9/M3gI8LH/yy9Ffv2VvvgVK/g44uzWiY0VqVxZpFQp+ljDibZt\n+RnExfELe+211B+raVP6wWNjRW65hY90qcVyF82ZI7JyJf+fN8+x/YEHXF0xd9xB15IzO3aISwzH\nmXvu4ZOeRXw8ffGVK6deZsXnQN0ygSUujinSI0bwSffvv+myfvxxPqHnyCHy3nv8/QQNNhsVR7ly\nCe9K5cqJXLzoOn7JEm4bONCxLi6ObqPffhMZN47+5mHDguyNJkPFisxCEqGya9kydcfZuVNcAp8v\nvMCbhXtAxmZL2tVm8e23PN6ePQxgGiPywQeOY9x8My8wi/fe48VmWRJXrtCVFhkpcvBgwuP3789j\nnj7NG3bHjjzf4MEpf++K31DlHsTs2CHSoQM//bFjAy2NB2w2kcOH6Yf9+muRN99MPHj64IMMspYt\nK5I3L5WD803Byux49FH67YOd69eZfvrOO1zu3DnxCUPJ0bs3j2Up7l9/5Wcxe7bruIEDuf6bb5I+\n3ksv0SqwbpS33SbSujX/P3iQxxgxwjF+wwauGzeOyrpFC34/kyZ5Pv5vvznGN2ggN4Kz4RhXCWFU\nuQc5NptI3bqMtSU2MTAk2LePCr5TJ1qm/fqJjBwpsmAB0zavXxf59FNeas2bJ7T+gw3LbTFhApc/\n+ojLKZU7JoZB54cfdqy7fp1Tmbt1c6zbvZs3xxw5qHjHjUv8mI0bMxXLomtXphuKMJLvnttuszGg\n2aoVXTbJWRNXr/LJwhhmZE2blqK3rKQPqtxDgLVr+Q306RNoSdKBceNoxd91V/oFHMaOZcZGbKz3\n+1hKct06Ls+cyeWUzk6bNIn7LVniuv6xx6j0Y2OpfFu2ZB74f/8xP90Yx43FGZtNpEAB15TE4cN5\njsOH+aQRFUXXizMvvOB4iho1Knm5W7fmDWjlypS9XyXdUOUeInTtSsNtz55AS5IOzJ3LyTzVq/vf\nRfPbbw4XUb16VJ7eMGgQ9zl/nstWjZPvv09+34sXRVatomukYkXGKdxjDTNm8HjLljn+HzqU2y5f\npnUeESEyebLrfkePcuzw4Y51q1fLjQBrs2ZMVXVn9Wrmmw8b5t37P3cu7email9R5R4iHDrEGdzO\nT+9hzezZvOzefNN/5zh3jn7ysmU5KzR3bs4R+O675P3H3bq5zkqNjaVyfOutpPd7+mnXeEOhQiI/\n/ZRw3KVLvMF1787smTvvdH2yuHRJpGFDukUOH3ast/z1y5Y51l2+zBvBu+8y3pFYkS13a14JaVS5\nhxADBsiNuScZgp49qQiXL0/ZfmfPUlknZ/V36UIX0Jo1XN6/X6RRI37IjRoxHpCYkq9dO+FM0ipV\n6LNODMu/1qkTn04OHUr6JmL5vxObILV7N5V2796OddaEKneX1u238ynBm4CsEhaocg8hLl9m3Kt0\naZH332c9qLTOeA9qLl3i9PlixVxrjuzbRwvZk387Ls4x09ZZ6bkzZQrH9O+fcP8vv6S1DDAHfNIk\n12nENhutfPe6OR078ikgMVq1oh/d28j4uHGSbDnbTp0YZLVcJN27M/ruzhNPOG4UGzd6d34lpFHl\nHmIsXUojzPnJvn59kUWLwjQTbf16BgAfeYTuhxdecNQiyZ2baXzOWPVZatTg3wULEh5z/37uW7du\n4kHUa9dYU6VSJR7nvfcc2yy/tnM6oQhzyY3x7N6w6rQ45/onx+XLfFxL6g7+11+uN6maNT1XqBw5\nkuOyZmU2jhL2qHIPUc6fp3u1f3+WfLHqV6XUgxESWPndmTJR0T/zDN0UJUvSEv77b46zMlaefpoK\ntkoV+rSda5b89Ret65w56dZIjvh4WuTZsjl821Z9FffmJT/8wPWbNiU8TuvW9HdbAVhf0rq1SP78\nfCLIlk3k1VcTjlmzhrLVqeP78ytBiSr3MODqVZGvvmJ8D2D11LDKqomLo3+8e3fXbJbdu/mmCxVi\nADZXLpFatfiBiHCqb9asnJQTHy8ycSKVX9Gijs5A3vDff7yxWO6RUaP4QR844Dpu61aud89giY7m\n+g8/TPl79wYrG8ZKZ/SUAx8Tw3Srl1/2jwxK0OFT5Q6gJYCdAHYDeNvD9hIAlgPYBGALgFbJHVOV\nu/fExLDses6c1GGffJIBnsB37KByB1iJ0l3hWoq4Xj3+bdiQbpWU8tJLDF7u2EEF6TwD1OLaNQZo\n+/Z1Xd+mDXPCPZW89RUNGzp8dYnl2q9fr+mLGQifKXcAkQD+A1AGQGYAfwGo5DbmGwDP2f+vBGBf\ncsdV5Z5yDh7kZFCA/vlt2wItkZ/ZsoXKzdOEGpuNRbgAFvZK7d3uxAk+GTz4IJ8Eqlf3PO7WW12L\nfllT+90Dt75mwQKex5jw734Vohw/zukQiXHhgm8TJLxV7t6U/K0FYLeI7BGR6wCmAWjrXlwSwE32\n/3MDOOJNRUolZRQrxmqts2axdHCXLqwiG7bcfjvLFzdsmHCbMcCUKSyF/PnnLNGbGgoWZMnj2bPZ\nV/TWWz2Pq1SJtZvfeAOoXZuvPHmAl15K3Xm9pUULoFo1oGJFIHt2/55LSTEXLgD16/Mr2rAh4faN\nG4GiRVld+fbbgWeeYdvZw4f9L5s3yv0WAAedlg/Z1znzAYDHjDGHAMwH8KKnAxljehpjoo0x0SdP\nnkyFuArAmvEjR7K8+pgxgZYmgGTNCtxxR9qP88orrJEfE5O4cq9Wjb/IESPYVPzNN9lQOk+etJ8/\nKYwB5s4Nz7r5QcrBg7x39+jBcviXL3seJ8Ke33v38jJ44AGW2LfYv58tAfLlAz74gMbZ9OlAt278\nSv1OcqY9gIcBjHFa7gpgpNuY1wD0tv9fF8B2ABFJHVfdMmnDZuN8nPz51d3qE775hu4PT7NKRZil\ns25d+NWuVxLQuzdDLLlzy40s0wcfZFjGGSvsM2gQPYi5crECxMWLnL5x2208xtatjn3i45kElpby\nSvChz70ugEVOy30A9HEbsw1AcaflPQAKJXVcVe5p56+/GAsM517V6UZ8PGu0hH2kWkmKixepkDt2\nZBx9yRLG3PPmZUWIgQN5iWzcyCSlli0d8ff58/l7bNOGfU8yZXKtFuErfKnco+zKujQcAdXKbmMW\nAHjC/v9toM/dJHVcVe6+oVcvXlCbNwdaEkUJfSxr3D2j9uhRRw+GO+/klIpbbklogY8YITcmISZW\nNj+teKvcDccmjTGmFYBhYObMOBEZaIwZYD/JXGNMJQDfAsgJBlffFJHFSR2zZs2aEh0dney5laQ5\ncwaoUIHxvpUr6aJVlIyGzQacOgUUKpT6Y4gAlSszbm21o3Vn1izg+eeBkyfZQrhBg4THGDKEfvYe\nPVIvS1IYYzaISM1kB3pzB/DHSy133zF6NC2FJ5/URvNKeHL5Mq1p51JAzjz1FLNFH3nE1cedEqzC\nm8lVdz53LrBpyPBhKqQS5PRdSGzRAAAgAElEQVToAbz2GlOsypcHPvkEuHo10FIpSsq4epWZKtZr\n925mg7VpA+TPz5TDJ5+kle7M2LEc16QJ8MsvTDns2BHYsSPpc7kfZ8QIWv6PPJK0nLlz80k56PHm\nDuCPl1ruvueffxjMAURKlWKRRUVJC/Pni/z+u//Ps2UL+3u792UHWGropZc4gRgQefFFRzE9K7B5\n77206k+d4kTiXLk4edhT3/FTp1iQ9LbbWIFVhBUvjHGtIxesQGvLZFyWLGEDkA4dAi2JEsqcPs00\nwIgIkcGD/VeddN06ZqMULcp+7GPG8DV2LDPCrPPabExTtIp5njnDMtmeApv//EOl76kJTteurFNX\npgyP1aoV+7dHRbn2RwlWVLlncPr357f722+BlkQJVaz+IE2b8u+jj7IUf2JcuMD2r1Z9N29YuZJW\ndunS3nVCtNlEevSgPOXKUSF76nci4ig6OmuWY90vvzhuDlevigwZInLTTVzXubP3cgcSb5W7V9ky\n/kCzZfzLlSucsV6oECP/ERpdUVKACCfr5s8PrF4NDB4M9OlDf/bs2UDp0q7jr10DWrUCli1jWYyJ\nExNmm6xcCcyYwYnFViWFzz4DSpQAlizhDE5viI8HOnUCfvwRGD488QoQsbFAjRrA6dPA9u1cV6UK\nfeYbNgBZsnDdiRPAt98Cjz0GlCzpnQyBRLNlFJk8mRbJd98FWhIl1FiyhNfOhAmOdQsW0I9doICr\nH94qjQ84Yj7uBTSnTKGVnS0bX5Y/vWZNFt5KKdevsxhmcq6itWvpVnr2WXZ3jIjgulAG6pZRbDa2\nBC1ShDPvFMVb2rdnaQv3ags7d4qUL8/ZmuPH8xrr1YuaxPLLP/20uLR0HT6cy40aOaoj22xMb0yP\nLmOvvuq4mbzxhv/P529UuSsiIvLnnw5LKizb9Sk+5/Bh1lZ5/XXP20+fZg9xgNPsrarL1vUVG8tp\n+ZGRbAULsDZLoMryXLpEn36FCp47JYYaqtyVG3Tu7LBcMmdmAKtsWQampk5N3WOxEpps3Sry7rtJ\nP8kNGMBr5d9/Ex9z/Tq7IgJspuXe3+TCBZFq1bj9qacSb2mbXpw549+eKumJt8pdA6oZgPPnOdHj\n/HkGvq5dA/btA1asAM6d45hChYCoKL4iIxkwa9SIr9q1GQRTggOR1JWZ2L4duOceTp2vVYsTfgoU\ncB0TF8fvvlIlYNGi5OXYsoVT9qOiEm4/dQpYtQpo21bLYvgSbwOqqtwzMPHxbCawdCmwZw+X4+P5\nA9+6lT9cEWYV9OwJDBwI5MoVaKkzNrGxQPPmQN68wNSpjoyP5Ni1izdqAOjblz1HSpYEFi9mtorF\nnDnsFzBzJtCune/lV9KOt8rdw/1WyShERgJ33cWXJ86eZT+KuXPZHGTOHGD0aKBly/SVU3EwaBCf\nuACgQwemFmbO7Dpm61YgRw4q7chI3ribNOGNe8UKWuV33MFp/fXr87vdupXpiH/8wZTEBx5I73em\n+BxvfDf+eKnPPbRYvZptRAGRxx4TOX8+0BJlPDZtYjphp04iX33F76J9e4c/e9s2BjKt+EqWLCJV\nqnBaf758nO3pzObNrlP+q1ZlNsn27en/3hTvgZc+d7XcFa+oV49t/QYOBD7+GDh+nD7b1LYuVVLG\n9etsz1agAPDll5xcdO0aC8Z17cr1o0bRbTZ4MN02O3cC//zD/4cOTdiR8M47OZknOhqoW5ftZJXw\nQZW74jVZsgADBgClSrES5Qsv0E2jwTLf8uWXwPjxVObdunFG5UcfMQYyZw4VOwC8+ioVfJ8+nIH8\n7LNA//4Jg6RJUbQo3TNK+KEBVSVV9O1LC37wYAbnFN/wyy/0dxcqxKejHDmAhx4CpkzhtP7vv0+4\nz5w5QLlyzFpRwh9vA6pacURJFR9+yLrXb77JoJ7iPefPA59+ynRUZ3bsYM2UqlUZBI2OZl3yH38E\nbr4ZGDbM8/HatlXFriRELXcl1cTEAE2bAps20WVQvnygJQp+Ll4EWrQA/vwTyJYN6NePfvNLlzif\n4MIFKvXixR37nD3LxhKWO0bJ2KjlrvidbNmAn37i/598ElhZQoHLl4HWrYF164Cvv2ZKaZ8+DGy2\naQPs388enc6KHWBAVBW7klJUuStpokgR4Kmn2OLv4MFAS+M7Dh8GXnyRgWPr9d57dKm4c+EC8PLL\nVNSXL3s+XkwM3SerVgGTJwPPPMOJQvPmMSi6ejUVfr16/n1fSgbCm3xJf7w0zz182LeP+dcvv5xw\nW2ysyNmz6S9TWoiLE2nQgHV4ihVzvCIi2C1o7lzH2BUr2NLQGOaKly7NcrkW8fEiq1ax4YUxriV0\nLa5cYbs4RfEGaINsJb0oWRLo3JkND06edKy/dg24916gbFm6HIKNy5c5fcedoUM5M/ebb1wbNq9Z\nQ/dImzYMfL72GtC4MWeBrlrFZhRRUXzPPXowi6hUKeDuu2mZjxnDnHR3smUDqlXz+9tVMhre3AH8\n8VLLPbzYvp2W6bvvcjk+XuSRR2jNZssmUr9+4CsDWthsIqNHcwZnu3asYGixZQst9gcf9Fwi+do1\nkQ8/5BhA5LnnXCssXrki8tZbLHebKZNI69Yikya5nkNR0gK05K+S3rRrx049589zGjsg8sknVG6A\nSL9+gZaQirhLF8pTowaV8G23sQnFtWsid94pUqhQwobL7uzcmXjvThHWRD9zxreyK4qIKnclAKxb\nxyuqfn2HVWtZv48/Tp/1ypWBk2/bNipyY2h9x8eLLF3KjkO5c/PmBLj61BUl2PBWuavPXfEZd91F\nf/Pq1ZxlOWKEozTByJFAmTKcZXnmTPrKdfUqp+VXr85myb/+Crz7LqfsN2nCvPLSpZmG2KOHVkRU\nwgNV7opPGT6cNU+mTnVt4JArF9cdP86gYmys784ZFwf88AMnAd18M/DEE0wzvHSJ9cpvvx344APW\nKd+8mROvnClVijek77+n/IoSDng1Q9UY0xLAcACRAMaIyCdu24cCaGxfzA6gkIjkSeqYOkM1YzJq\nFPD888yumTCBmSap5fhxTs3/4gtg717WV6lRgwr97FlWrIyN5czZr74CmjXz3ftQlEDhs2YdxphI\nAF8BaAbgEID1xpi5IrLdGiMirzqNfxGAJnYpHnnuObb2e+cdFsVKSVXJq1eBSZOYpvjHH8Du3Vxf\npw4wZAgnCUVG0pJfvZoThAoV4mQkbROoZDS8KflbC8BuEdkDAMaYaQDaAtieyPhOAN73jXhKONKn\nD2usDBpEd82QIVTwixczN7xZM5a4dVbIBw+y7duGDaw7Xr8+W/81bkxr3fkGERXl6P+qKBkVb5T7\nLQCcJ5YfAlDb00BjTEkApQEsS7toSjgzcCB94l98QaV++jRw9CiQPTuLkM2fD0ycSMW9ciVbyl27\nxqCnNlxWlOTxdbOORwHMEJF4TxuNMT0B9ASAEs5deZUMhzFU7EePskdrbCybTMTGctvhw3S3dOjA\nYGn58sDs2UDFioGWXFFCA2+yZQ4DcK5TV8y+zhOPApia2IFE5BsRqSkiNQtqT68MS3w8MH0665bP\nmAFUqQJs3MjSBQcOMF3ywgUgXz5m2Nx/P7B2rSp2RUkJ3ij39QDKG2NKG2Mygwp8rvsgY8ytAPIC\n+NO3IirhxMKFTE189FHWKJ8yhSVwq1bl9ptuojtmwgTgyhUuN24MZM6c8Fh79zKwqihKQpJV7iIS\nB6AXgEUAdgD4QUS2GWMGGGOcuy8+CmCaeJNbqWRIfvyR9cxFgGnTgL//ZgEuT+mQXbuyCUiNGsyb\nL1+ehbxiYuieadmSBcnuvhs4ciT934uiBDvaiUnxKf/9x+bOTz7JWZ8W06fT3VK3LoOluXJ5dzwR\nYNkyzihds8aRu37LLZxJ+vXXwHffceKSomQEtBOTEhCee45pjBUqcCr/nj30m3fuzPTFBQu8V+wA\ng6tNm9L98ssvvGnMns3+o199BRQuDCxa5Le3oyghi6+zZZQMzJIlrNvyzjvMY//mG07pFwEaNuSk\nohw5UndsY4BWrfhypnlzPgnYbKwVoygK0Z+D4hNsNuDtt4ESJdiObsQIWu0vvkirPS2KPSlatGCO\n/MaNvj+2ooQyarkrPmHGDM4eHT/eMbO0aFF2NfInVr2YRYuAmsl6IRUl46CWu5JmYmOBvn2Zr/7Y\nY+l77kKF2KLOk9/95ZeBbt04s1VRMhqq3JU0M3Ysi3h9/HHaqjymlhYtgD//5MQniw0b6BqaMIE9\nT69cSf448R7nVSuKGz//DLRvH/RWgyp35QZHj7Lq4tNP868ndu1icLRhQ6Y2vv02G2HUr88c9kDQ\nogUrQS5zqmj03nuc4Tp8OIO899/PIG9ibNjALJ5ff/W/vClm4UL6u5TAEx8PvP46GwYMGxZoaZLG\nm3ZN/nhpm73AcOmSSJ8+IlWritSpI9KkCZs4V6nCFnMAG0cDIkOHuu67e7fILbewLV2DBiKlSolE\nRbEP6erVgXk/Iux9mjOnyLPPcnnVKsr/6adcnjKFMtatK3LunOdjNG/OferW9dwYO2CcOsUegMaI\nrF+f/udfvpzdzxUyYwYvlBIlRHLkEDl4MN1FgPZQVdyZPZvXJECl3ry5yN13i1SvLtKsGZXhhg0i\nV6+KPPQQx33xBffdt4/75ssn8tdfjmPGx7PpdKB54AGRMmX4f+PGIoUL80Zm8dNPIpkyibRokVB5\nr1zJ91qtGv+uWOFnYfftE6lQQaRoUcerRg2RAwcSjn3tNTafzZdPpHZtfuDOnDvHL++rr5I+58mT\nIuPH84utUYN36uTYtUskc2Z+mMeOef/+EmPQIF543pw7GLHZRGrWFClXTuTff0WyZhXp2DHdxVDl\nrsiFCzT2Jk2idQ7QQv/tt+T3vX5dpH177tO3LxVnnjxU/sHIl19S1tGj+XfEiIRjRo7ktlGjHOts\nNj6FFLk5Xk53ekEKFYiT5s1TcOJLl0RmzRJ56imRb7/1bp8hQyhI9+7cr0cPPno0aCASG+sYt2cP\nleuTT4pMmMB9xo51bI+LE7nvPq43RuSXXxKea9s2kUaNeIMAeCPJk0ekdGmRQ4cSl9FmE2nZUiRX\nLiqxFi0S3lhSwpQpPH9EBJ9EZs1K/bECxZIlfA/ffMPlDz7g8pIl6SqGKvcMhM0mMnEijbyHH6aB\nd/PNcsPNAlB3DBlCpe0t16+LdOjA/XPlElmzxn/vIa3s2kU5M2USKV6cTx/u2Gx8WsmeneNFRBYt\n4n4j75snAsigWjMFEImOTuaEa9aI3H+/w4cVGUkf1aZNyQvbtKlI5cqu6yzl/cEHjnWdO4tky8ZH\nf5uNj1kFCoicOcPtb7zheLyqVk3kpptcXSh//kmLv1AhkX79+KZsNt7xc+USqVSJFr0nZs1yHHvU\nKP7/2WfJvzdPREfzBtGggcg//9D6BUR6907ZBRlo7r1XpEgRx8V15Qpvkrfdlq7vQ5V7BuJ//+M3\nmS0bn/abNqVR+PHHdEds2+ZZ2XlDbCyfptet863MvsZm4+8MEPlmVCwfV2rX5iPLkSM3xh06JJI3\nL+MNsbEid90lUrJEvFzNe7NIZKScy1RAct8UL+3bJ3GymBjeQQoXFnn5ZZFly+i2KFyYSjapH/rF\ni7wDvf56wm1du9KyXbmSChEQeecdx/bNm7n9hRccN4Pnn+e2/fupxMuVo/JfsIB3sbJlRf77L+G5\nVqygwq1ZU+T8eddtV66IlCzJG9D16/xwH3qINy/nC+H4cX7Offpwe+XK/AxeeYWuJxGRo0cZqClZ\nUuTECa67epVyAyKtWnl+Ijh6lIGhxG4o06aJ3HqryJYtnrcnx4YNIvPn87OJi+O6EydEvvtOpF07\nfr/vvcfPQoQ3REBk8GDX48ydy/VDhiR+rj/+EFm4UGTv3rQ9/dhR5Z5B2LWLv+FmzXxy3YQ077x+\nTW4vckKuF7Nr+YoV+eEULkwFbGfqVG62gqhjH7Gb75MmiURESN+6S8QYhxFss1EXzJ1r1wOffsrx\nTscUEZGZM7l+4MDEhZw9m2OWLk247cIFKudixUTq16eV7h4B7tWLCj5LFgYXnG8kq1fzxnHHHVTE\nVasm7SufN4/jatYU+fVXRzCiXz9JEHw4fZoKr0wZkU8+oXzGcFxUFK2KBx5w3AQiI/nkUbs2v4PN\nmxOef/hwcYl8W8TH84K2Hjvd4wmW3ACt5suXE3+Pnpgxw+GmsjIIypZ1vJ9ixWilA3y/8+fzkTh3\n7oQ3QhHeoG66yfFE5czevQ5ZAd5Qb79d5McfUyazE6rcMwCxsbRA8+RJ2n2aITh8WKRKFbEB9DH/\n/DOVxNattPAiIkQ++ujGHfCRR3j1lytrk9iiJUTuuUesDSdylJJs2Wzy8MMin3/O36L127yzSqys\nyNGKLhlPdOxIP/m2bZ63P/MMfWTXrnneHh1NBZ1Y4ODsWVropUszk8adsWO57z33JJ4a5MxPPzl8\neFWrMniRJYtIp04Jx/7+u0MpVq9OF1J0dMInlQMH6CPMmZNjZ8zwfG6bjYGdqCiRtWsd6wcP5n4j\nR4q0acP/J07ktuXLqSBr1HBkrjz9dPLv02LhQn6+9erx5vXtt3yKeughvp+NGx03uaVLee1YX77z\nU5Qzmzdz+3vvJdzWowevh59/pq++d28+TS5Y4L3MbqhyzwB89BG/wSlTAi1JgNm5k4/9OXN6/tFc\nvEgrEuCPOCZGTp+mcbjwtYVcv3Ahx27cKALIy/XW3fhN16pF19fkySIlcp0WQKR9s3Oyd68HWY4f\nZ65o7dqOx30Lm40pR23bJv1+xo2jFZzYDeDgQVrSibF5c8r8cFeviowZ41BkOXPyZumJv/7ynNXj\nibNnk49BnDnDz6RMGVrFa9dS2bdvz88rJoZPKJGR9DPmzOkaK3jrLcr8ww/Jy7NqFX2Xd95J2bzh\n2jX6JevV43ebGO3bM47h/L389x/l7tXLu3N5iSr3MGfjRv4GHnkk0JIEmOhokYIF6cJIKg/cZmPi\nvmXZnztHK75iRVqszvmRLVrImQLl5fNPrrsa4Hv2yJWoXDKg+izJnp1P7zExHs5lZYZYeaQW27Zx\n/ddfp+EN+5H4eLogvEmn8iWrV1MJtm9PJV+ihKuL48IF3mGBhFk+169zW+7cdIEcPUpr/IEH+HTx\nyCN0M339NceUL++btE53tmyRG6llFt278ykosRtlKlHlHsZcukTjpUiRpA24kMAK2HnixAkGDxNL\nXZk/n5ZcyZK03r1h8mSHP9rKApk2zXXM8uVc/7//ua7v1ImW36FDsnQphwwf7uEcNhv9sDlz3gjm\nLl4scnGg3ce8f793smYkrMfQyEha2O6cPs3sIE/B4f/+o887b1658bhVqhTTN8uUcfjSixf372ff\noQO/85MnmQcfGcmAu49R5R6m2Gwijz3G63Xx4kBLk0auXKEroHHjhP7hc+doeQH0Wf7vf46bQFwc\nLSSAj9gpDThYmSQAA2nOueUiPE/t2lQG/frxZaUdvvvujWGNGtFdbSVUuPDvv5S7SxeZPt1u1JWa\nxLuykpC4OPrOR49O3f6zZ/ML+egjWtHOBkNMDGMvFy74RNRE2bqVP8y33xbp1o2xAadMLV+hyj3E\nWbOGbkR3o9YyNgcMCIxcKebcucSnr1tZGVFRVNJHj3L95cvMiY6KYgaLNVGnUyfObmzcmMs9eiSi\nWb1gzRpadVOnet6+cCGVs/NkgcqVXbIlVqwQj96XG7z7rhxDIcmf+zpnrGOfxL/mIQVSCR8efZSG\nQ2Qkg8p+QJV7CLNtG2MzAJMyrJv/unXUN/fdFyJpjzabSMOGzE5wf9TevduRlbFoEet0lCnDG8F9\n99ECstwl8fG0yKxMjWzZmI8cBDRpwkxLT9l4tkuXpV22BZLZXJN3H97B7MkhQTrFV/EN27fz2s2W\nzT++fVHlHrKcPk1PQeHC1GdZs3KS4dixjDOVLOk5Ay4omTxZbkxvLVTI1d/5wAOuWRlr1zLLJDJS\nXKZ4O7N0KYNuqZ244gd+/10SncNixVU/xRty+eYykgvn5YmucQkHKuHFoEGpdy95gSr3ECQ2lrNL\nM2fmpDYRkR07OIvScj0HojCgR2w2puQtWcLJO+65zufPM+JbsyZ9kTfdxCDmpUuchOJpOvuOHfSz\nDxuWfu/DB9x7LxN2nAuVHTlizYS1SVyzliKA9CixWHLmdB2nKClFlXsI8tJL/EbGjXNdHxtLfTdn\nTjoJcuKEY6q4M1evUrg6dehGcfZHt27t6pvo3Zvrrckpv/zCx9X27floks71OPzJ6tV8qy1aMAzw\n+OMMIWTNylIqsnOnSM6csrLvIgFYOUBRUou3yt1wbPpTs2ZNiY6ODsi5g4UDB9jYedMmYP16YMEC\n4JVX/N93NEni4oDy5YH9+4G6ddnGqFkzdrEYPpwdPW6/HWjcGLj1VqBiRWDrVgperx671Bw9Ctx5\nJ3vcjRnjOPZnnwFvvsn/lywBmjYNzHv0A926sadGVBSQKROQOTPw7rvA44/bB1y9ClumLChX3qBs\n2SBtCqKEBMaYDSKSfMdgb+4A/nhlBMv9wAHPxumFC7TuLKM3IoKG7CuvJMzKS3es2iePPcYp3s7W\n+b33MvjpKS/9xx/pN6pShdUL8+RJaP3bbEwp9FQ0K4Pw/vt8gPF2kqeiuAN1ywSGq1eZvVenjtyY\nUDdmjEPJr1/P2lARESJvvsmqrEHlg23RglX8rLvMwYMi33/vXSnbJUsc9USSax6RQdm9mx/Pxx8H\nWhIlVFHlHgCGDmVgDWCRvP79HaWrS5XiZMtMmThtfeXKdBRs2zY+FiRXPc/SPM41xVPKxo1M83Gv\nq6Lc4O67OXfL23Z+168nXmZGyXiock9nrHLPjRtz5qiVh26zMZZolcZ46KF0LhkQF+e4wyRXPe+N\nN5iKmOFLTPqXb7/l15EnDxOKypRhRtT777tOrtyxg/Ng8ufnk6CiiHiv3L0KqBpjWgIYDiASwBgR\n+cTDmI4APgAgAP4Skc5JHTPcAqr33QesWwfs3QvcdFPC7SLAnj1AmTKAMeko2LffAj17Mtj5xx/A\nDz8AHTokHHf1KlCsGNCoEfDTT+koYMbj6lXg00+B06eBmBi+9u8HVq/mdVK+PFCoEJejohi33roV\n2LkTqFAh0NIrgcZnAVVQof8HoAyAzAD+AlDJbUx5AJsA5LUvF0ruuOFkua9aRUvMvedAwDl9mmZf\nw4Z8rq9d21E9z52JE/kmfv013cVUyLFjLF7YrBnj0oMGcd2BA+qnVxzAV5a7MaYugA9EpIV9uY/9\npjDIacxgALtEZIznoyQkFC1366Nyt7ybNAG2bwf++w/IkSP95UqU556j5b5pE9MX9+4FqlYFKlUC\nfvuNOXsW9esDJ08C//wDREQETmbFI3XqALGxwIYNgZZECTTeWu7e/IpvAXDQafmQfZ0zFQBUMMas\nNsassbtxPAnV0xgTbYyJPnnypBenDi5efpmPyM73pGXLgOXLgXfeCTLFvnEjMHo00KsXFTsAlC4N\nfPMNsGYN8MILfO4/dQrYsoUum2efVcUepLRvz690795AS6KECt5Y7g8DaCkiT9mXuwKoLSK9nMbM\nAxALoCOAYgB+A3C7iJxL7LihZrlv3QrccQcQGUn9N2wYdeHdd3My0r//AlmzBkCwa9eASZOA8eOB\nXLk4qahiReC774B9++iozZPHdZ/nnwdGjXIsZ87MN3X4MJAvX3pKr3jJnj1A2bLAkCFA796BlkYJ\nJN5a7lFeHOswgOJOy8Xs65w5BGCtiMQC2GuM2QX64dd7KW/Q8847DJSuW8fJmM8/D0ydSoP366/T\nQbHHxgIXLzqWY2KACROAESOAY8eAypWBS5eAlSuBK1c45vvvEyp2APjqK2qInTsdrxo1VLEHMWXK\nANWqATNmqHJXvCQ5pzx4A9gDoDQcAdXKbmNaAvje/n8B0I2TP6njhlJA1ar8ZwW0nCvQlirlxxzk\nU6dYiOThhx2Tg9xfzZu7dq6Pj2cEbsMG7xOplZDAalZ08GDiY06fZj22UaPYUOrYMb0Mwg34OBWy\nFYBhYObMOBEZaIwZYD/JXGOMAfC5XcnHAxgoItOSOmaouGVEgAYN+Fi8ezeQPbtj26ZNQM6cTF0D\nAOzYARQoABQsmPyB16xhwLNTJ+CZZ4DcuR0nXLmSz98LFgA2G1CkCGu83HabI5prDNCwIWu4KBmC\nnTsZ8xk+HHjpJddt8fHA2LF8wjx92nVbsWLA4sW8fJTQR2vL+Ii5c8W7nsarVnH6ae7cnHqf3AzN\njh0dtctz5WIVxQkTHBOOChYU6dOHHTpCojOHkh5UrszMVmdWr3aUAWrYUGTzZj68LVrEaqK5c4u0\nbRsYeRXfA52hmnbi4vhjKl8+meq0+/ezGUXZsizIDvDXtm6d5/Fnz7ILUa9edJ906uRQ9OXL806S\n2vZxSljTrx8Ljx07xkundWteNkWLsjmIJxeM5c6xegQooY0q9zRw7pzI9OkiDz7IT+iHH5IYfOmS\nSLVqtL63b+eva9o0zis3htUS3bHmn1u1zkU4sWj5cq3JoiTJli28dCpV4t+8eUUGDhS5eDHxfS5e\nZGevRo0SKv8vvxRp1cq1SZYS3KhyTwUHD4q0bEnvCiBSoABreyQakLLZRDp0oBL/5RfXbefPs2ND\n2bIJ6/g2bMjKYhrpUlKIzSZy++1sbPX++zREvOHLL3lNL1zoWDd2LNcZw4nMztuU4EWVeyp48kl2\nz3nzTbrQkzWiBw3iRzh4sOftc+ZIgtZKe/dy3Ucf+UpsJYNx5gxth5Rw7Rozu6pXZwhn1ixme7Vo\nwS6IVapQyffvryGeYEeVewo5eJAW+4sverlDTAwjVW3aJG6B22z0vZcu7XDaf/ghP3ZP9V0UxY9M\nmMBL79VXGfKpXdvRS+DSJfZnAUS6dg2snErSeKvcda65naFDmXX42mte7vDzz8D585zen1iZR2OA\nDz7gnPEJE5jmOHEiUy+Agv4AAA/OSURBVBhLlfKR5IriHZ07A1Wq8FovUwb45RdHyYwcOXiJvvUW\nL9FVqwIrq5J2VLkDOHOGZVg6dUqBzp0wAShalFXDkuL++4G77gI++oi1XHbtcmqsqSjpR2QkJyc3\nbw4sWgTkz++63RigXz9Oq3jrLUehPCU0UeUOXvCXLzt6NyfLyZPshtylC38xSWFZ7/v2cXzWrMDD\nD6dRYkVJHQ0bUrEXL+55e/bsQP/+LKsxZ076yqb4lgyv3C9f5oy/1q0dxROTZdo0IC4O6NrVu/H3\n3QfUrs0KY23aOGajKkoQ0r07Z8L26cPLXAlNMrxyHzeO07XffjsFO02YwLro3t4NjKFbxhjgqadS\nJaeipBdRUcCgQSzt/913iY/bto1VURcuTD/ZFO/xqraMPwiG2jKxsUC5ckCJEsDvv3sYsG8fA6fP\nPccrHuAVf9ttwOefpyD6aufUKdaeUZQgR4SKe+/ehDWVAJa4btiQBUlz5ODvp1q1wMia0fBls46w\npX9/ekreeSeRAW+8wQpN7duzxC7AVIKICEZfU4oqdiVEMAYYPBg4epR9C44fd2zbvx9o2pQum6VL\nWSn6/vv5W3Lm2DHWwFMChDf5kv54BTrPfdkyTtro3j2RASdOMPG9WjUObNCA9VRLlOA0VkXJAPTu\nzclO2bKJvPyyyPr1ImXKiOTJI7JpE8f8/TdnzFapwhmzR48ylz5rVubNz5kT2PcQbkAnMSXOyZMs\ntFSxomMSRwK++IIfz99/s1ZMpkwit9zCdZMnp6u8ihJIdu4UeeIJR227nDlF1qxxHbNkiUhUlMit\nt1KpR0ZynzvvZIHTY8cCI3s4oso9EWw2kQceEMmc2WF5eBxUubJIrVqOdYsXi+TIwSv78uV0kVVR\ngok9e1iaI7HqkuPHc+brE0+I/Psv123dynWtW2spJV+hyj0RRozgux4+PIlBa9Zw0OjRrut37BD5\n80+/yqcooYynekzDhiX8OW3dKtK5M28ER4+m/DwxMSIzZiSsyZcRUOXuge++o3clWSvi6adFsmdP\neXUmRVESEB8vcu+9/EktXCjSpQvDWLly0arPm1fk++9TZtm//z61V79+fhM7aPFWuWeIbJn4eM4+\n7d6d6VsTJyZeDgaXL3OSUocO7IitKEqaiIhgvnzmzEDLlsDMmfw97t0LbN4MVKoEdOsGtGoFHDqU\n/PGuXOGs8syZOX3EYxqzGxcuuPaXzwiEvXK/dAl46CHgs8+Yrr5gAZAnTxI7zJjBq6BHj3STUVHC\nnWLF+NN65x32I/7kE9a2ufVW4LffgBEjqKRbtqTyTooJEzhlZOZMFkDr0gU4ezbx8cuWcT5LzZrA\nuXO+fV9BjTfmvT9e6eGWOX2a9asjItiswCsaNNBGGooSABYupKulR4/Ex8TFiZQrJ3LXXfyJrlvH\nLJ2HH074k7XZ2GohIoLdKzNlYhZzqDc7Q0b3uZ87x17TmTOLzJvn5U7btvEj+eQTv8qmKIpn+vTh\nT3DSJM/bZ87k9unTHes+/dSRJLFrF/MetmyhwgfYLO3iRQZ0AZE33kif9+IvMrRyv3BBpE4d3ql/\n/tnLnY4epUmQJ0/qwveKoqSZ2FiRu+9m1vE//yTcXq8ee984Z8nExzv60ju/IiNFhgxxteiff57b\nJk70/3vxF94q96gAe4V8zuXLnAq9fj3w44+s9pgsZ8+yyPWRI8CSJcDNN/tdTkVREhIVBUydyrp8\nHTsCf/7pqGvzxx98jRjhKPUEMGA7axbLQNls3BYZSX++e22/YcNY8Oypp4DSpYH69dPvvaU3YVU4\n7MoV4IEHgBUrgClTgEce8WKnS5eAZs2AjRuBefP4v6IoAWXBAmbP3HQTK2Y/+CAweTL73Rw4AOTM\nmfpjnzwJ1KnD47z7LoO8mTL5TnZ/k+EKh1kW+4oVwPffe6nYY2OBdu2AdeuY/qiKXVGCgvvuY5ZL\nhw7A8uWs0zdvHjPe0qLYAaBgQSA6mjrigw+AevWA7dt9InZQERZumUuXqNhXrWKaVJcuXu44dSrd\nMN9+SyWvKErQ0LgxX/HxwNq1tNp79vTNsfPmBSZN4hPBs88C1asD8+cn3zUzlAh5t8ylS3x8W72a\nX5bXlXhFgBo1gGvXgK1bk5jVpChKOHP8OFXBHXdQwQc7PnXLGGNaGmN2GmN2G2MS9CwyxjxhjDlp\njNlsf6Vbu6HevRlkmTIlhSXWf/8d2LQJeOUVVeyKkoEpXBh44gn2lj1yJNDS+I5klbsxJhLAVwDu\nA1AJQCdjTCUPQ6eLSFX7a4yP5fTIlStU6o8/7qWP3ZlhwzhF7rHH/CKboiihQ7duzLSZODHQkvgO\nbyz3WgB2i8geEbkOYBqAtv4VyztmzaJbplu3FO64Zw8wezbwzDNAtmx+kU1RlNChfHm2FRw/nh7b\ncMAb5X4LgINOy4fs69xpb4zZYoyZYYwp7hPpkuH774GSJYEGDVK448iRTIR9/nm/yKUoSujxxBNs\nkbx2baAl8Q2+SoX8GUApEbkDwK8Avvc0yBjT0xgTbYyJPnnyZJpOePgwE10ef5yTGLzmwgVgzBjO\nkLjF0z1KUZSMSMeOnDA1fnygJfEN3qjFwwCcLfFi9nU3EJHTInLNvjgGQA1PBxKRb0SkpojULFiw\nYGrkvcGkSXx86to1mYF79gBbtjgaXI8fz6qPL7+cpvMrihJe5MoFtG/PKS+WukgJIsDffwP9+wdH\n1o03ee7rAZQ3xpQGlfqjADo7DzDGFBGRo/bFNgB2+FRKN0TokqlXj76yRDlwgPOYL15kRkyJEsD5\n89yxVi1/iqgoSgjSvTuDqrNnO7Lvrl4F/v0XqFzZs5fg3385v+bHH4GdO7mucGFg/34gS5b0k92d\nZC13EYkD0AvAIlBp/yAi24wxA4wxbezDXjLGbDPG/AXgJQBP+EtgANiwAdixI5lAqghnPNhswLhx\nnIpWvz47A3z4oT/FUxQlRGnUCChVig/4Bw+yNEHx4syBr1wZGD3aUW9+7Vpa+hUrAh9/TC/vqFEs\nk3D8ODB9eiDfCUKzKmSvXiJZstjk7M+/s9t1o0YJ26t/9x3Lv3ldyF1RFIUt/IxhVcmICJG2bdl7\nuXp1qpT8+Vl1FmAR2b59RY4ccexvs4lUqiRSrZp/2kLAy6qQITdD9fqVOBS9OR5No37D9LPNgQIF\neCstUgRYvJitWY4epYVepQqwcmUKI66KomRkDh1i97Z77mFCXalSXC/CuY9DhwK7dgFPP82Gbbly\nJTzGt9/ScbBiBZ8GnImPZ7JeavF2hmrIWe6zOkwWQGRekadERo0SuXJFZM0akXz5RG6+WWTzZt5q\ns2YV2bkzVedQFEVJC1eu0MJ/8EHX9UePitx5p8iCBak/NsK1nvvFOs1QNfocWuz4Gshiv/3Vrs2q\nYc2bs5bn1atsmlqhQmCFVRQlQ5ItGwuSffwxE/bKlGGp4aZNgX370l7Z0htCzi2TJAcPssNu/vys\nE5qWZx9FUZQ0cOQIJ1n26sW68U2aMLNm/ny6fFKLt26ZkLPck6R4cSaa2myq2BVFCShFi7Lm1dix\nDP3t3MluUWlR7Ckh/CKNERGuPbgURVECxKuvcprNtm2shZWe/YBUCyqKoviJGjUY/qtaFbj33vQ9\ntyp3RVEUP/L664E5b/i5ZRRFURRV7oqiKOGIKndFUZQwRJW7oihKGKLKXVEUJQxR5a4oihKGqHJX\nFEUJQ1S5K4qihCEBKxxmjDkJYH8qdy8A4JQPxfElwSpbsMoFBK9swSoXELyyBatcQPjIVlJEkm1C\nHTDlnhaMMdHeVEULBMEqW7DKBQSvbMEqFxC8sgWrXEDGk03dMoqiKGGIKndFUZQwJFSV+zeBFiAJ\nglW2YJULCF7ZglUuIHhlC1a5gAwmW0j63BVFUZSkCVXLXVEURUmCkFPuxpiWxpidxpjdxpi3AyzL\nOGPMCWPMVqd1+Ywxvxpj/rX/zRsAuYobY5YbY7YbY7YZY14OBtmMMVmNMeuMMX/Z5epvX1/aGLPW\n/p1ON8ZkTk+53GSMNMZsMsbMCxbZjDH7jDF/G2M2G2Oi7esCfp3Z5chjjJlhjPnHGLPDGFM30LIZ\nYyraPyvrdcEY80qg5XKS71X79b/VGDPV/rvw+XUWUsrdGBMJ4CsA9wGoBKCTMaZSAEUaD6Cl27q3\nASwVkfIAltqX05s4AL1FpBKAOgBesH9OgZbtGoAmInIngKoAWhpj6gD4FMBQESkH4CyAHukslzMv\nA9jhtBwssjUWkapO6XKB/i4thgNYKCK3ArgT/OwCKpuI7LR/VlUB1ABwBcCsQMsFAMaYWwC8BKCm\niFQBEAngUfjjOhORkHkBqAtgkdNyHwB9AixTKQBbnZZ3Aihi/78IgJ1B8LnNAdAsmGQDkB3ARgC1\nwckbUZ6+43SWqRj4o28CYB4AEwyyAdgHoIDbuoB/lwByA9gLe+wumGRzkqU5gNXBIheAWwAcBJAP\n7IQ3D0ALf1xnIWW5w/HBWByyrwsmCovIUfv/xwAUDqQwxphSAKoBWIsgkM3u9tgM4ASAXwH8B+Cc\niMTZhwTyOx0G4E0ANvtyfgSHbAJgsTFmgzGmp31dwL9LAKUBnATwnd2VNcYYkyNIZLN4FMBU+/8B\nl0tEDgMYAuAAgKMAzgPYAD9cZ6Gm3EMK4W04YOlIxpicAH4C8IqIXHDeFijZRCRe+LhcDEAtALem\ntwyeMMa0BnBCRDYEWhYP3C0i1UF35AvGmIbOGwN4nUUBqA5glIhUA3AZbq6OQP4G7H7rNgB+dN8W\nKLnsfv624I2xKIAcSOja9QmhptwPAyjutFzMvi6YOG6MKQIA9r8nAiGEMSYTqNgni8jMYJINAETk\nHIDl4CNoHmOM1aw9UN9pfQBtjDH7AEwDXTPDg0E2u7UHETkB+o5rITi+y0MADonIWvvyDFDZB4Ns\nAG+GG0XkuH05GOS6F8BeETkpIrEAZoLXns+vs1BT7usBlLdHljODj1xzAyyTO3MBdLP/3w30d6cr\nxhgDYCyAHSLyRbDIZowpaIzJY/8/GxgH2AEq+YcDJRcAiEgfESkmIqXA62qZiHQJtGzGmBzGmFzW\n/6APeSuC4DoTkWMADhpjKtpXNQWwPRhks9MJDpcMEBxyHQBQxxiT3f47tT4z319ngQp0pCEg0QrA\nLtBX2zfAskwF/WaxoBXTA/TTLgXwL4AlAPIFQK67wUfOLQA221+tAi0bgDsAbLLLtRVAP/v6MgDW\nAdgNPkJnCfD3eg+AecEgm/38f9lf26xrPtDfpZN8VQFE27/T2QDyBoNsoLvjNIDcTusCLpddjv4A\n/rH/BiYCyOKP60xnqCqKooQhoeaWURRFUbxAlbuiKEoYospdURQlDFHlriiKEoaoclcURQlDVLkr\niqKEIarcFUVRwhBV7oqiKGHI/wFjhmHKVPIZSAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YYTWjj76_GYN",
        "colab_type": "code",
        "outputId": "aa5076a7-9f04-4e25-b4a1-7aca60fec3a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        }
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-15-1209fb88269b>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    python -tt scriptname.py\u001b[0m\n\u001b[0m                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "woybl9dm7SBW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}